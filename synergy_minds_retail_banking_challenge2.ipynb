{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1L-OGlQNS66"
      },
      "source": [
        "# Retail Banking Challenge 2 - Baseline Submission\n",
        "\n",
        "This notebook provides a simple baseline for **Retail Banking Challenge 2: Credit Default Prediction**.\n",
        "\n",
        "**Goal**: Predict `DefaultLabel` (0/1) for each customer-week combination\n",
        "**Metric**: Macro-F1 - Higher is better\n",
        "\n",
        "## Instructions:\n",
        "1. **Replace API credentials** in the first cell with your team's API key and name\n",
        "2. **Run all cells** to generate and submit baseline predictions\n",
        "3. **Check the output** for your submission score\n",
        "\n",
        "This baseline uses only tabular customer panel data with a simple Random Forest classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "id": "RzrMdcb7NS68"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from agentds import BenchmarkClient\n",
        "\n",
        "# ðŸ”‘ REPLACE WITH YOUR CREDENTIALS\n",
        "client = BenchmarkClient(\n",
        "    api_key=\"adsb_E8N9aNAz2w1K7dNYT8BSMGsd_1760199769\",        # Get from your team dashboard\n",
        "    team_name=\"synergy-minds\"     # Your exact team name\n",
        ")\n",
        "# 1. Load all data sources\n",
        "print(\"ðŸ“‚ Loading all data sources...\")\n",
        "\n",
        "# Core customer data\n",
        "customers = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customers_all.csv\")\n",
        "accounts = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/accounts_all.csv\")\n",
        "devices = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/devices_all.csv\")\n",
        "\n",
        "# Transaction data\n",
        "transactions_train = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_train.csv\")\n",
        "transactions_test = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_test.csv\")\n",
        "\n",
        "# Panel data\n",
        "train_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_train.csv\")\n",
        "test_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_test.csv\")\n",
        "\n",
        "# Device sessions\n",
        "with open(\"/home/jovyan/shared/datasets/RetailBanking/device_sessions_all.json\", 'r') as f:\n",
        "    device_sessions = pd.json_normalize(json.load(f))\n",
        "\n",
        "print(\"âœ… Data loaded successfully!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egKggB9gNS6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnl7lTOdNS6-",
        "outputId": "af5a0be4-6d8a-4e0a-e017-c51fa281e8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Loading data...\n",
            "ðŸŽ¯ Creating advanced temporal and behavioral features...\n",
            "ðŸ”„ Creating advanced feature sets...\n",
            "âœ… Advanced features created:\n",
            "   Customer features: (1000, 27)\n",
            "   Temporal features train: (13301, 21)\n",
            "   Temporal features test: (13290, 20)\n",
            "   Transaction features: (1000, 32)\n",
            "   Session features: (1000, 20)\n",
            "ðŸŽ¯ Final dataset: 97 features, 13301 samples\n",
            "ðŸ” Selecting top features...\n",
            "   Selected 40 most predictive features\n",
            "ðŸ¤– Training highly optimized model...\n",
            "ðŸ“Š Top 15 most important features:\n",
            "                     feature  importance\n",
            "35             total_actions    0.214580\n",
            "6                CreditScore    0.214220\n",
            "37    sensitive_action_ratio    0.122896\n",
            "17    financial_health_score    0.065299\n",
            "1              HardInquiries    0.056378\n",
            "5          low_payment_weeks    0.042956\n",
            "8                total_limit    0.036592\n",
            "14           limit_to_salary    0.029893\n",
            "24           txn_period_days    0.029396\n",
            "36    financial_action_ratio    0.028632\n",
            "9                  max_limit    0.024105\n",
            "31           large_txn_ratio    0.023289\n",
            "22                 first_txn    0.018936\n",
            "28             avg_large_txn    0.018682\n",
            "38  comprehensive_risk_score    0.015122\n",
            "ðŸŽ¯ Analyzing threshold options:\n",
            "   Threshold 0.15: Default rate = 0.150 (1993 defaults)\n",
            "   Threshold 0.2: Default rate = 0.126 (1674 defaults)\n",
            "   Threshold 0.25: Default rate = 0.113 (1508 defaults)\n",
            "   Threshold 0.3: Default rate = 0.094 (1244 defaults)\n",
            "   Threshold 0.35: Default rate = 0.084 (1123 defaults)\n",
            "âœ… Selected threshold: 0.35\n",
            "âœ… Final predictions saved: 13290 predictions\n",
            "   Default rate: 0.084 (1123 defaults)\n",
            "âœ… Prediction submitted successfully!\n",
            "ðŸ“Š Score: 0.8032 (Macro-F1)\n",
            "âœ… Validation passed\n",
            "ðŸŽ‰ Submission successful!\n",
            "   ðŸ“Š Score: 0.8032\n",
            "   ðŸŽ¯ Need improvement: 0.1278 to reach top score\n",
            "\n",
            "ðŸ’¡ Key improvements in this approach:\n",
            "   â€¢ Temporal patterns and trends from panel data\n",
            "   â€¢ Comprehensive risk scoring from multiple sources\n",
            "   â€¢ Advanced behavioral features from transactions and sessions\n",
            "   â€¢ Optimized feature selection and model parameters\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load all data\n",
        "print(\"ðŸ“‚ Loading data...\")\n",
        "customers = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customers_all.csv\")\n",
        "accounts = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/accounts_all.csv\")\n",
        "transactions_train = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_train.csv\")\n",
        "transactions_test = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_test.csv\")\n",
        "train_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_train.csv\")\n",
        "test_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_test.csv\")\n",
        "\n",
        "with open(\"/home/jovyan/shared/datasets/RetailBanking/device_sessions_all.json\", 'r') as f:\n",
        "    device_sessions = pd.json_normalize(json.load(f))\n",
        "\n",
        "print(\"ðŸŽ¯ Creating advanced temporal and behavioral features...\")\n",
        "\n",
        "def create_customer_segmentation_features(customers, accounts):\n",
        "    \"\"\"Create customer segmentation and financial health features\"\"\"\n",
        "    features = customers.copy()\n",
        "\n",
        "    # Encode categorical variables\n",
        "    le_city = LabelEncoder()\n",
        "    features['HomeCity_encoded'] = le_city.fit_transform(features['HomeCity'].fillna('Unknown'))\n",
        "\n",
        "    # Account-based features\n",
        "    account_stats = accounts.groupby('CustomerID').agg({\n",
        "        'Balance': ['sum', 'mean', 'std', 'max'],\n",
        "        'Limit': ['sum', 'max'],\n",
        "        'AccountID': 'count',\n",
        "        'Type': lambda x: (x == 'credit_card').sum()\n",
        "    }).reset_index()\n",
        "    account_stats.columns = ['CustomerID', 'total_balance', 'avg_balance', 'std_balance', 'max_balance',\n",
        "                            'total_limit', 'max_limit', 'num_accounts', 'num_credit_cards']\n",
        "\n",
        "    # Credit utilization\n",
        "    account_stats['overall_utilization'] = np.where(\n",
        "        account_stats['total_limit'] > 0,\n",
        "        account_stats['total_balance'] / account_stats['total_limit'],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # Account type distribution\n",
        "    account_types = pd.get_dummies(accounts[['CustomerID', 'Type']], columns=['Type'], prefix='account')\n",
        "    account_types = account_types.groupby('CustomerID').sum().reset_index()\n",
        "\n",
        "    # Merge features\n",
        "    features = features.merge(account_stats, on='CustomerID', how='left')\n",
        "    features = features.merge(account_types, on='CustomerID', how='left')\n",
        "\n",
        "    # Financial health indicators\n",
        "    features['balance_to_salary'] = features['total_balance'] / (features['AnnualSalary'] + 1)\n",
        "    features['limit_to_salary'] = features['total_limit'] / (features['AnnualSalary'] + 1)\n",
        "\n",
        "    # Risk flags\n",
        "    features['high_utilization_flag'] = (features['overall_utilization'] > 0.8).astype(int)\n",
        "    features['multiple_credit_cards'] = (features['num_credit_cards'] > 2).astype(int)\n",
        "    features['low_credit_score'] = (features['CreditScore'] < 600).astype(int)\n",
        "\n",
        "    # Customer segmentation\n",
        "    features['premium_customer'] = ((features['AnnualSalary'] > features['AnnualSalary'].quantile(0.7)) &\n",
        "                                   (features['CreditScore'] > 700)).astype(int)\n",
        "    features['risky_customer'] = ((features['CreditScore'] < 580) |\n",
        "                                 (features['overall_utilization'] > 0.9)).astype(int)\n",
        "\n",
        "    # Financial health score (FIXED: This was missing)\n",
        "    features['financial_health_score'] = (\n",
        "        (features['CreditScore'] / 850) * 0.4 +\n",
        "        (1 - features['overall_utilization'].clip(0, 1)) * 0.3 +\n",
        "        (1 - (features['balance_to_salary'].clip(0, 2) / 2)) * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_temporal_sequence_features(panel_df):\n",
        "    \"\"\"Create time-series features from panel data\"\"\"\n",
        "    temporal_features = []\n",
        "\n",
        "    for customer_id in panel_df['CustomerID'].unique():\n",
        "        customer_data = panel_df[panel_df['CustomerID'] == customer_id].sort_values('Week')\n",
        "\n",
        "        if len(customer_data) > 1:\n",
        "            # Rolling statistics\n",
        "            for window in [3, 5]:\n",
        "                customer_data[f'utilization_ma_{window}'] = customer_data['Utilisation'].rolling(window=window, min_periods=1).mean()\n",
        "                customer_data[f'payment_ratio_ma_{window}'] = customer_data['PaymentRatio'].rolling(window=window, min_periods=1).mean()\n",
        "                customer_data[f'utilization_std_{window}'] = customer_data['Utilisation'].rolling(window=window, min_periods=1).std()\n",
        "\n",
        "            # Trends and changes\n",
        "            customer_data['utilization_trend'] = customer_data['Utilisation'].diff().rolling(window=3, min_periods=1).mean()\n",
        "            customer_data['payment_ratio_trend'] = customer_data['PaymentRatio'].diff().rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "            # Volatility measures\n",
        "            customer_data['utilization_volatility'] = customer_data['Utilisation'].rolling(window=4, min_periods=1).std()\n",
        "            customer_data['payment_volatility'] = customer_data['PaymentRatio'].rolling(window=4, min_periods=1).std()\n",
        "\n",
        "            # Cumulative risk indicators\n",
        "            customer_data['cumulative_inquiries'] = customer_data['HardInquiries'].cumsum()\n",
        "            customer_data['high_utilization_weeks'] = (customer_data['Utilisation'] > 0.7).cumsum()\n",
        "            customer_data['low_payment_weeks'] = (customer_data['PaymentRatio'] < 0.2).cumsum()\n",
        "\n",
        "            # Recent deterioration indicators\n",
        "            customer_data['recent_utilization_increase'] = (customer_data['Utilisation'] > customer_data['Utilisation'].shift(1)).astype(int)\n",
        "            customer_data['recent_payment_decrease'] = (customer_data['PaymentRatio'] < customer_data['PaymentRatio'].shift(1)).astype(int)\n",
        "\n",
        "        else:\n",
        "            # Single week - use current values\n",
        "            customer_data['utilization_ma_3'] = customer_data['Utilisation']\n",
        "            customer_data['payment_ratio_ma_3'] = customer_data['PaymentRatio']\n",
        "            customer_data['utilization_std_3'] = 0\n",
        "            customer_data['utilization_trend'] = 0\n",
        "            customer_data['payment_ratio_trend'] = 0\n",
        "            customer_data['utilization_volatility'] = 0\n",
        "            customer_data['payment_volatility'] = 0\n",
        "            customer_data['cumulative_inquiries'] = customer_data['HardInquiries']\n",
        "            customer_data['high_utilization_weeks'] = (customer_data['Utilisation'] > 0.7).astype(int)\n",
        "            customer_data['low_payment_weeks'] = (customer_data['PaymentRatio'] < 0.2).astype(int)\n",
        "            customer_data['recent_utilization_increase'] = 0\n",
        "            customer_data['recent_payment_decrease'] = 0\n",
        "\n",
        "        temporal_features.append(customer_data)\n",
        "\n",
        "    return pd.concat(temporal_features, ignore_index=True).fillna(0)\n",
        "\n",
        "def create_transaction_behavior_features(transactions_df):\n",
        "    \"\"\"Create comprehensive transaction behavior features\"\"\"\n",
        "    transactions = transactions_df.copy()\n",
        "    transactions['Timestamp'] = pd.to_datetime(transactions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Basic transaction stats\n",
        "    txn_stats = transactions.groupby('CustomerID').agg({\n",
        "        'TxnID': 'count',\n",
        "        'Amount': ['sum', 'mean', 'std', 'max', 'min'],\n",
        "        'Timestamp': ['min', 'max', 'nunique']\n",
        "    }).reset_index()\n",
        "    txn_stats.columns = ['CustomerID', 'txn_count', 'total_amount', 'avg_amount', 'amount_std',\n",
        "                        'max_amount', 'min_amount', 'first_txn', 'last_txn', 'txn_days']\n",
        "\n",
        "    # Transaction period and velocity\n",
        "    txn_stats['txn_period_days'] = (txn_stats['last_txn'] - txn_stats['first_txn']).dt.days + 1\n",
        "    txn_stats['daily_txn_rate'] = txn_stats['txn_count'] / txn_stats['txn_period_days']\n",
        "    txn_stats['daily_spending'] = txn_stats['total_amount'] / txn_stats['txn_period_days']\n",
        "\n",
        "    # Large transaction behavior\n",
        "    large_txn_threshold = transactions['Amount'].quantile(0.85)\n",
        "    large_txns = transactions[transactions['Amount'] > large_txn_threshold]\n",
        "    large_txn_stats = large_txns.groupby('CustomerID').agg({\n",
        "        'TxnID': 'count',\n",
        "        'Amount': 'mean'\n",
        "    }).reset_index()\n",
        "    large_txn_stats.columns = ['CustomerID', 'large_txn_count', 'avg_large_txn']\n",
        "\n",
        "    # Channel preferences\n",
        "    channel_dummies = pd.get_dummies(transactions[['CustomerID', 'Channel']],\n",
        "                                   columns=['Channel'], prefix='channel')\n",
        "    channel_means = channel_dummies.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "    # MCC category spending\n",
        "    if 'MCC_Group' in transactions.columns:\n",
        "        mcc_dummies = pd.get_dummies(transactions[['CustomerID', 'MCC_Group']],\n",
        "                                   columns=['MCC_Group'], prefix='mcc')\n",
        "        mcc_means = mcc_dummies.groupby('CustomerID').mean().reset_index()\n",
        "    else:\n",
        "        mcc_means = pd.DataFrame(columns=['CustomerID'])\n",
        "\n",
        "    # Temporal patterns\n",
        "    transactions['hour'] = transactions['Timestamp'].dt.hour\n",
        "    transactions['day_of_week'] = transactions['Timestamp'].dt.dayofweek\n",
        "    transactions['is_weekend'] = (transactions['day_of_week'] >= 5).astype(int)\n",
        "    transactions['is_night'] = ((transactions['hour'] >= 22) | (transactions['hour'] <= 6)).astype(int)\n",
        "\n",
        "    temporal_patterns = transactions.groupby('CustomerID').agg({\n",
        "        'is_weekend': 'mean',\n",
        "        'is_night': 'mean',\n",
        "        'hour': ['mean', 'std']\n",
        "    }).reset_index()\n",
        "    temporal_patterns.columns = ['CustomerID', 'weekend_txn_ratio', 'night_txn_ratio',\n",
        "                                'avg_txn_hour', 'std_txn_hour']\n",
        "\n",
        "    # Merge all transaction features\n",
        "    features = txn_stats.merge(large_txn_stats, on='CustomerID', how='left')\n",
        "    features = features.merge(channel_means, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in mcc_means.columns:\n",
        "        features = features.merge(mcc_means, on='CustomerID', how='left')\n",
        "    features = features.merge(temporal_patterns, on='CustomerID', how='left')\n",
        "\n",
        "    # Create risk scores\n",
        "    features['large_txn_ratio'] = features['large_txn_count'] / (features['txn_count'] + 1)\n",
        "    features['spending_volatility'] = features['amount_std'] / (features['avg_amount'] + 1)\n",
        "    features['transaction_consistency'] = features['txn_days'] / (features['txn_period_days'] + 1)\n",
        "\n",
        "    features['spending_risk_score'] = (\n",
        "        features['large_txn_ratio'] * 0.4 +\n",
        "        features['spending_volatility'] * 0.3 +\n",
        "        features['night_txn_ratio'] * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_session_risk_features(device_sessions):\n",
        "    \"\"\"Create session-based risk features\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "    sessions['Timestamp'] = pd.to_datetime(sessions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Basic session stats\n",
        "    session_stats = sessions.groupby('CustomerID').agg({\n",
        "        'SessionID': 'count',\n",
        "        'City': 'nunique',\n",
        "        'DeviceID': 'nunique',\n",
        "        'Timestamp': ['min', 'max']\n",
        "    }).reset_index()\n",
        "    session_stats.columns = ['CustomerID', 'session_count', 'unique_cities', 'unique_devices',\n",
        "                            'first_session', 'last_session']\n",
        "\n",
        "    # Session frequency\n",
        "    session_stats['session_period_days'] = (session_stats['last_session'] - session_stats['first_session']).dt.days + 1\n",
        "    session_stats['daily_sessions'] = session_stats['session_count'] / session_stats['session_period_days']\n",
        "\n",
        "    # Action analysis\n",
        "    def analyze_actions(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'total_actions': len(actions_list),\n",
        "                'financial_actions': 0,\n",
        "                'financial_amount': 0,\n",
        "                'logins': 0,\n",
        "                'sensitive_actions': 0\n",
        "            }\n",
        "            for action in actions_list:\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', '')\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['financial_amount'] += action.get('amount', 0)\n",
        "                    if action_type in ['transfer', 'payment', 'account_view']:\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                elif action == 'login':\n",
        "                    stats['logins'] += 1\n",
        "            return stats\n",
        "        return {'total_actions': 0, 'financial_actions': 0, 'financial_amount': 0,\n",
        "                'logins': 0, 'sensitive_actions': 0}\n",
        "\n",
        "    action_stats = sessions['Actions'].apply(analyze_actions)\n",
        "    action_df = pd.DataFrame(action_stats.tolist())\n",
        "    action_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    # Aggregate action stats\n",
        "    action_agg = action_df.groupby('CustomerID').agg({\n",
        "        'total_actions': 'sum',\n",
        "        'financial_actions': 'sum',\n",
        "        'financial_amount': 'sum',\n",
        "        'logins': 'sum',\n",
        "        'sensitive_actions': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate ratios\n",
        "    action_agg['financial_action_ratio'] = action_agg['financial_actions'] / (action_agg['total_actions'] + 1)\n",
        "    action_agg['sensitive_action_ratio'] = action_agg['sensitive_actions'] / (action_agg['total_actions'] + 1)\n",
        "    action_agg['avg_financial_amount'] = action_agg['financial_amount'] / (action_agg['financial_actions'] + 1)\n",
        "\n",
        "    # Merge session features\n",
        "    features = session_stats.merge(action_agg, on='CustomerID', how='left')\n",
        "\n",
        "    # Session risk scores\n",
        "    features['geographic_risk'] = (features['unique_cities'] > 3).astype(int)\n",
        "    features['device_risk'] = (features['unique_devices'] > 2).astype(int)\n",
        "    features['activity_risk'] = (features['daily_sessions'] > features['daily_sessions'].quantile(0.8)).astype(int)\n",
        "\n",
        "    features['session_risk_score'] = (\n",
        "        features['geographic_risk'] * 0.4 +\n",
        "        features['device_risk'] * 0.3 +\n",
        "        features['sensitive_action_ratio'] * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "print(\"ðŸ”„ Creating advanced feature sets...\")\n",
        "\n",
        "# Create all feature sets\n",
        "customer_features = create_customer_segmentation_features(customers, accounts)\n",
        "temporal_train = create_temporal_sequence_features(train_panel)\n",
        "temporal_test = create_temporal_sequence_features(test_panel)\n",
        "transaction_features = create_transaction_behavior_features(pd.concat([transactions_train, transactions_test]))\n",
        "session_features = create_session_risk_features(device_sessions)\n",
        "\n",
        "print(\"âœ… Advanced features created:\")\n",
        "print(f\"   Customer features: {customer_features.shape}\")\n",
        "print(f\"   Temporal features train: {temporal_train.shape}\")\n",
        "print(f\"   Temporal features test: {temporal_test.shape}\")\n",
        "print(f\"   Transaction features: {transaction_features.shape}\")\n",
        "print(f\"   Session features: {session_features.shape}\")\n",
        "\n",
        "# Combine all features\n",
        "def create_final_features(panel_df, customer_df, temporal_df, transaction_df, session_df):\n",
        "    \"\"\"Combine all feature sources\"\"\"\n",
        "    # Start with temporal panel\n",
        "    features = temporal_df.copy()\n",
        "\n",
        "    # Add customer features\n",
        "    features = features.merge(customer_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Add transaction features\n",
        "    features = features.merge(transaction_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Add session features\n",
        "    features = features.merge(session_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Create powerful interaction features (FIXED: Check if columns exist)\n",
        "    risk_components = []\n",
        "\n",
        "    # Check each component exists before using it\n",
        "    if 'utilization_ma_3' in features.columns:\n",
        "        risk_components.append(features['utilization_ma_3'] * 0.2)\n",
        "\n",
        "    if 'payment_ratio_ma_3' in features.columns:\n",
        "        risk_components.append((1 - features['payment_ratio_ma_3']) * 0.2)\n",
        "\n",
        "    if 'financial_health_score' in features.columns:\n",
        "        risk_components.append((1 - features['financial_health_score']) * 0.2)\n",
        "\n",
        "    if 'spending_risk_score' in features.columns:\n",
        "        risk_components.append(features['spending_risk_score'] * 0.2)\n",
        "\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        risk_components.append(features['session_risk_score'] * 0.2)\n",
        "\n",
        "    # Create comprehensive risk score if we have components\n",
        "    if risk_components:\n",
        "        features['comprehensive_risk_score'] = sum(risk_components) / len(risk_components)\n",
        "    else:\n",
        "        features['comprehensive_risk_score'] = 0\n",
        "\n",
        "    # Deterioration score\n",
        "    deterioration_components = []\n",
        "    if 'utilization_trend' in features.columns:\n",
        "        deterioration_components.append(features['utilization_trend'] * 0.4)\n",
        "\n",
        "    if 'recent_payment_decrease' in features.columns:\n",
        "        deterioration_components.append(features['recent_payment_decrease'] * 0.3)\n",
        "\n",
        "    if 'cumulative_inquiries' in features.columns:\n",
        "        deterioration_components.append((features['cumulative_inquiries'] / 10) * 0.3)\n",
        "\n",
        "    if deterioration_components:\n",
        "        features['deterioration_score'] = sum(deterioration_components)\n",
        "    else:\n",
        "        features['deterioration_score'] = 0\n",
        "\n",
        "    # Financial stress indicators\n",
        "    stress_components = []\n",
        "    if 'overall_utilization' in features.columns:\n",
        "        stress_components.append((features['overall_utilization'] > 0.8).astype(int) * 0.4)\n",
        "\n",
        "    if 'balance_to_salary' in features.columns:\n",
        "        stress_components.append((features['balance_to_salary'] > 0.5).astype(int) * 0.3)\n",
        "\n",
        "    if 'CreditScore' in features.columns:\n",
        "        stress_components.append((features['CreditScore'] < 600).astype(int) * 0.3)\n",
        "\n",
        "    if stress_components:\n",
        "        features['financial_stress'] = sum(stress_components)\n",
        "    else:\n",
        "        features['financial_stress'] = 0\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "# Create final datasets\n",
        "X_train_final = create_final_features(train_panel, customer_features, temporal_train, transaction_features, session_features)\n",
        "X_test_final = create_final_features(test_panel, customer_features, temporal_test, transaction_features, session_features)\n",
        "\n",
        "# Prepare for modeling\n",
        "y_train = X_train_final['DefaultLabel'].astype(int)\n",
        "non_feature_cols = ['CustomerID', 'Week', 'DefaultLabel']\n",
        "feature_cols = [col for col in X_train_final.columns if col not in non_feature_cols]\n",
        "\n",
        "X_train = X_train_final[feature_cols]\n",
        "X_test = X_test_final[feature_cols]\n",
        "\n",
        "print(f\"ðŸŽ¯ Final dataset: {X_train.shape[1]} features, {X_train.shape[0]} samples\")\n",
        "\n",
        "# Ensure numeric types\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Feature selection - keep top 40 features\n",
        "print(\"ðŸ” Selecting top features...\")\n",
        "k = min(40, X_train.shape[1])\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
        "\n",
        "print(f\"   Selected {len(selected_features)} most predictive features\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "print(\"ðŸ¤– Training highly optimized model...\")\n",
        "\n",
        "# Optimized Random Forest\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=25,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=0.7,\n",
        "    class_weight='balanced_subsample',\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': selected_features,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"ðŸ“Š Top 15 most important features:\")\n",
        "print(feature_importance.head(15))\n",
        "\n",
        "# Generate predictions with multiple threshold options\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Try different thresholds and analyze\n",
        "threshold_options = [0.15, 0.2, 0.25, 0.3, 0.35]\n",
        "best_threshold = 0.25\n",
        "\n",
        "print(\"ðŸŽ¯ Analyzing threshold options:\")\n",
        "for threshold in threshold_options:\n",
        "    preds = (y_pred_proba > threshold).astype(int)\n",
        "    default_rate = preds.mean()\n",
        "    print(f\"   Threshold {threshold}: Default rate = {default_rate:.3f} ({preds.sum()} defaults)\")\n",
        "\n",
        "    # Prefer thresholds that give 5-12% default rate (typical for credit risk)\n",
        "    if 0.05 <= default_rate <= 0.12:\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(f\"âœ… Selected threshold: {best_threshold}\")\n",
        "\n",
        "# Final predictions\n",
        "predictions = (y_pred_proba > best_threshold).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'CustomerID': test_panel['CustomerID'],\n",
        "    'Week': test_panel['Week'],\n",
        "    'DefaultLabel': predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"retailbanking_challenge2_final_predictions.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Final predictions saved: {submission_df.shape[0]} predictions\")\n",
        "print(f\"   Default rate: {predictions.mean():.3f} ({predictions.sum()} defaults)\")\n",
        "\n",
        "# Submit\n",
        "try:\n",
        "    from agentds import BenchmarkClient\n",
        "    client = BenchmarkClient(api_key=\"adsb_E8N9aNAz2w1K7dNYT8BSMGsd_1760199769\", team_name=\"synergy-minds\")\n",
        "\n",
        "    result = client.submit_prediction(\"Retailbanking\", 2, \"retailbanking_challenge2_final_predictions.csv\")\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"ðŸŽ‰ Submission successful!\")\n",
        "        print(f\"   ðŸ“Š Score: {result['score']:.4f}\")\n",
        "        if result['score'] < 0.9:\n",
        "            print(f\"   ðŸŽ¯ Need improvement: {0.931 - result['score']:.4f} to reach top score\")\n",
        "        else:\n",
        "            print(\"   ðŸ† Excellent score!\")\n",
        "    else:\n",
        "        print(\"âŒ Submission failed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ’¥ Submission error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Key improvements in this approach:\")\n",
        "print(\"   â€¢ Temporal patterns and trends from panel data\")\n",
        "print(\"   â€¢ Comprehensive risk scoring from multiple sources\")\n",
        "print(\"   â€¢ Advanced behavioral features from transactions and sessions\")\n",
        "print(\"   â€¢ Optimized feature selection and model parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lPQIfD9NS7A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUlurimbNS7A",
        "outputId": "864fe29f-6d0f-4caa-f0c0-80c9b3b25245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Loading and preparing data for elite performance...\n",
            "ðŸŽ¯ Creating elite feature engineering pipeline...\n",
            "ðŸ”„ Building elite feature sets...\n",
            "ðŸŽ¯ Creating final feature matrix...\n",
            "âœ… Elite feature matrix: 118 features, 13301 samples\n",
            "ðŸ” Performing elite feature selection...\n",
            "ðŸŽ¯ Selected 50 most predictive features\n",
            "ðŸ¤– Training elite ensemble model...\n",
            "ðŸ“Š Analyzing optimal threshold...\n",
            "âœ… Optimal threshold: 0.10\n",
            "   Expected default rate: 0.137\n",
            "ðŸŽ‰ Elite predictions saved: 13290 predictions\n",
            "   Default rate: 0.137 (1826 defaults)\n",
            "âœ… Prediction submitted successfully!\n",
            "ðŸ“Š Score: 0.9185 (Macro-F1)\n",
            "âœ… Validation passed\n",
            "ðŸ† Submission successful!\n",
            "   ðŸ“Š Score: 0.9185\n",
            "   ðŸ’ª Excellent! You're in the top tier!\n",
            "\n",
            "ðŸ’¡ Elite strategy features:\n",
            "   â€¢ Advanced temporal dynamics with trend analysis\n",
            "   â€¢ Sophisticated customer segmentation and risk profiling\n",
            "   â€¢ Comprehensive transaction behavior intelligence\n",
            "   â€¢ Session-based security and risk indicators\n",
            "   â€¢ Ensemble modeling with optimized threshold selection\n",
            "   â€¢ Elite feature selection focusing on predictive power\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ðŸš€ Loading and preparing data for elite performance...\")\n",
        "\n",
        "# Load data\n",
        "customers = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customers_all.csv\")\n",
        "accounts = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/accounts_all.csv\")\n",
        "transactions_train = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_train.csv\")\n",
        "transactions_test = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_test.csv\")\n",
        "train_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_train.csv\")\n",
        "test_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_test.csv\")\n",
        "\n",
        "with open(\"/home/jovyan/shared/datasets/RetailBanking/device_sessions_all.json\", 'r') as f:\n",
        "    device_sessions = pd.json_normalize(json.load(f))\n",
        "\n",
        "print(\"ðŸŽ¯ Creating elite feature engineering pipeline...\")\n",
        "\n",
        "def create_elite_customer_features(customers, accounts):\n",
        "    \"\"\"Create sophisticated customer segmentation and risk profiling\"\"\"\n",
        "    features = customers.copy()\n",
        "\n",
        "    # Advanced encoding\n",
        "    le_city = LabelEncoder()\n",
        "    features['HomeCity_encoded'] = le_city.fit_transform(features['HomeCity'].fillna('Unknown'))\n",
        "\n",
        "    # Account portfolio analysis\n",
        "    account_metrics = accounts.groupby('CustomerID').agg({\n",
        "        'AccountID': 'count',\n",
        "        'Balance': ['sum', 'mean', 'std', 'max', 'min', 'median'],\n",
        "        'Limit': ['sum', 'max', 'mean'],\n",
        "        'Type': lambda x: x.nunique()\n",
        "    }).reset_index()\n",
        "    account_metrics.columns = ['CustomerID', 'total_accounts', 'balance_sum', 'balance_mean',\n",
        "                              'balance_std', 'balance_max', 'balance_min', 'balance_median',\n",
        "                              'limit_sum', 'limit_max', 'limit_mean', 'account_type_diversity']\n",
        "\n",
        "    # Credit-specific features\n",
        "    credit_accounts = accounts[accounts['Type'] == 'credit_card']\n",
        "    if not credit_accounts.empty:\n",
        "        credit_metrics = credit_accounts.groupby('CustomerID').agg({\n",
        "            'Balance': ['sum', 'mean', 'max'],\n",
        "            'Limit': ['sum', 'max', 'mean']\n",
        "        }).reset_index()\n",
        "        credit_metrics.columns = ['CustomerID', 'credit_balance_sum', 'credit_balance_mean',\n",
        "                                 'credit_balance_max', 'credit_limit_sum', 'credit_limit_max',\n",
        "                                 'credit_limit_mean']\n",
        "\n",
        "        credit_metrics['credit_utilization'] = credit_metrics['credit_balance_sum'] / (credit_metrics['credit_limit_sum'] + 1)\n",
        "        credit_metrics['max_credit_utilization'] = credit_metrics['credit_balance_max'] / (credit_metrics['credit_limit_max'] + 1)\n",
        "    else:\n",
        "        credit_metrics = pd.DataFrame(columns=['CustomerID', 'credit_utilization', 'max_credit_utilization'])\n",
        "\n",
        "    # Account type composition\n",
        "    account_composition = pd.get_dummies(accounts[['CustomerID', 'Type']], columns=['Type'], prefix='account')\n",
        "    account_composition = account_composition.groupby('CustomerID').sum().reset_index()\n",
        "\n",
        "    # Merge features\n",
        "    features = features.merge(account_metrics, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in credit_metrics.columns:\n",
        "        features = features.merge(credit_metrics, on='CustomerID', how='left')\n",
        "    features = features.merge(account_composition, on='CustomerID', how='left')\n",
        "\n",
        "    # Advanced financial ratios\n",
        "    features['balance_to_salary'] = features['balance_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['limit_to_salary'] = features['limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['credit_depth'] = features['credit_limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "\n",
        "    # Risk profiling\n",
        "    features['high_risk_profile'] = (\n",
        "        (features['CreditScore'] < 580).astype(int) * 0.4 +\n",
        "        (features.get('credit_utilization', 0) > 0.8).astype(int) * 0.3 +\n",
        "        (features['balance_to_salary'] > 0.5).astype(int) * 0.3\n",
        "    )\n",
        "\n",
        "    # Customer lifetime value proxy\n",
        "    features['clv_score'] = (\n",
        "        (features['AnnualSalary'] / features['AnnualSalary'].max()) * 0.4 +\n",
        "        (features['CreditScore'] / 850) * 0.3 +\n",
        "        (features['Tenure'] / features['Tenure'].max()) * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_temporal_dynamics_features(panel_df):\n",
        "    \"\"\"Create sophisticated time-series features with trend analysis\"\"\"\n",
        "    temporal_features = []\n",
        "\n",
        "    for customer_id in panel_df['CustomerID'].unique():\n",
        "        cust_data = panel_df[panel_df['CustomerID'] == customer_id].sort_values('Week')\n",
        "\n",
        "        # Rolling statistics with multiple windows\n",
        "        for window in [2, 3, 4]:\n",
        "            cust_data[f'utilization_ma_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'payment_ratio_ma_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'utilization_std_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).std()\n",
        "            cust_data[f'payment_ratio_std_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).std()\n",
        "\n",
        "        # Trend analysis\n",
        "        cust_data['utilization_trend_3'] = cust_data['Utilisation'].diff(periods=2).fillna(0)\n",
        "        cust_data['payment_trend_3'] = cust_data['PaymentRatio'].diff(periods=2).fillna(0)\n",
        "\n",
        "        # Acceleration (second derivative)\n",
        "        cust_data['utilization_acceleration'] = cust_data['utilization_trend_3'].diff().fillna(0)\n",
        "        cust_data['payment_acceleration'] = cust_data['payment_trend_3'].diff().fillna(0)\n",
        "\n",
        "        # Volatility measures\n",
        "        cust_data['utilization_volatility'] = cust_data['Utilisation'].rolling(4, min_periods=1).std()\n",
        "        cust_data['payment_volatility'] = cust_data['PaymentRatio'].rolling(4, min_periods=1).std()\n",
        "\n",
        "        # Behavioral patterns\n",
        "        cust_data['high_utilization_streak'] = (cust_data['Utilisation'] > 0.7).astype(int)\n",
        "        cust_data['low_payment_streak'] = (cust_data['PaymentRatio'] < 0.2).astype(int)\n",
        "\n",
        "        # Calculate streaks\n",
        "        for i in range(1, len(cust_data)):\n",
        "            if cust_data.iloc[i]['high_utilization_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('high_utilization_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['high_utilization_streak'] + 1\n",
        "            if cust_data.iloc[i]['low_payment_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('low_payment_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['low_payment_streak'] + 1\n",
        "\n",
        "        # Deterioration indicators\n",
        "        cust_data['financial_deterioration'] = (\n",
        "            (cust_data['utilization_trend_3'] > 0).astype(int) * 0.5 +\n",
        "            (cust_data['payment_trend_3'] < 0).astype(int) * 0.5\n",
        "        )\n",
        "\n",
        "        temporal_features.append(cust_data)\n",
        "\n",
        "    return pd.concat(temporal_features, ignore_index=True).fillna(0)\n",
        "\n",
        "def create_advanced_transaction_features(transactions_df):\n",
        "    \"\"\"Create comprehensive transaction behavior profiling\"\"\"\n",
        "    transactions = transactions_df.copy()\n",
        "    transactions['Timestamp'] = pd.to_datetime(transactions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Basic transaction metrics\n",
        "    txn_metrics = transactions.groupby('CustomerID').agg({\n",
        "        'TxnID': 'count',\n",
        "        'Amount': ['sum', 'mean', 'std', 'max', 'min', 'median', 'skew'],\n",
        "        'Timestamp': ['min', 'max', 'nunique']\n",
        "    }).reset_index()\n",
        "    txn_metrics.columns = ['CustomerID', 'txn_count', 'amount_sum', 'amount_mean', 'amount_std',\n",
        "                          'amount_max', 'amount_min', 'amount_median', 'amount_skew',\n",
        "                          'first_txn', 'last_txn', 'active_days']\n",
        "\n",
        "    # Transaction period and velocity\n",
        "    txn_metrics['txn_period_days'] = (txn_metrics['last_txn'] - txn_metrics['first_txn']).dt.days + 1\n",
        "    txn_metrics['daily_txn_frequency'] = txn_metrics['txn_count'] / txn_metrics['txn_period_days']\n",
        "    txn_metrics['daily_spending'] = txn_metrics['amount_sum'] / txn_metrics['txn_period_days']\n",
        "\n",
        "    # Large transaction analysis\n",
        "    large_txn_threshold = transactions['Amount'].quantile(0.8)\n",
        "    large_txns = transactions[transactions['Amount'] > large_txn_threshold]\n",
        "\n",
        "    if not large_txns.empty:\n",
        "        large_txn_stats = large_txns.groupby('CustomerID').agg({\n",
        "            'TxnID': 'count',\n",
        "            'Amount': ['mean', 'sum', 'max']\n",
        "        }).reset_index()\n",
        "        large_txn_stats.columns = ['CustomerID', 'large_txn_count', 'large_txn_avg',\n",
        "                                  'large_txn_sum', 'large_txn_max']\n",
        "\n",
        "        large_txn_stats['large_txn_ratio'] = large_txn_stats['large_txn_count'] / txn_metrics['txn_count']\n",
        "        large_txn_stats['large_amount_ratio'] = large_txn_stats['large_txn_sum'] / txn_metrics['amount_sum']\n",
        "    else:\n",
        "        large_txn_stats = pd.DataFrame(columns=['CustomerID', 'large_txn_ratio', 'large_amount_ratio'])\n",
        "\n",
        "    # Channel behavior\n",
        "    channel_behavior = pd.get_dummies(transactions[['CustomerID', 'Channel']],\n",
        "                                    columns=['Channel'], prefix='channel')\n",
        "    channel_behavior = channel_behavior.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "    # MCC spending patterns\n",
        "    if 'MCC_Group' in transactions.columns:\n",
        "        mcc_behavior = pd.get_dummies(transactions[['CustomerID', 'MCC_Group']],\n",
        "                                    columns=['MCC_Group'], prefix='mcc')\n",
        "        mcc_behavior = mcc_behavior.groupby('CustomerID').mean().reset_index()\n",
        "    else:\n",
        "        mcc_behavior = pd.DataFrame(columns=['CustomerID'])\n",
        "\n",
        "    # Temporal patterns\n",
        "    transactions['hour'] = transactions['Timestamp'].dt.hour\n",
        "    transactions['day_of_week'] = transactions['Timestamp'].dt.dayofweek\n",
        "    transactions['is_weekend'] = (transactions['day_of_week'] >= 5).astype(int)\n",
        "    transactions['is_night'] = ((transactions['hour'] >= 22) | (transactions['hour'] <= 6)).astype(int)\n",
        "\n",
        "    temporal_patterns = transactions.groupby('CustomerID').agg({\n",
        "        'is_weekend': 'mean',\n",
        "        'is_night': 'mean',\n",
        "        'hour': ['mean', 'std', lambda x: x.mode()[0] if len(x.mode()) > 0 else 12]\n",
        "    }).reset_index()\n",
        "    temporal_patterns.columns = ['CustomerID', 'weekend_ratio', 'night_ratio',\n",
        "                                'avg_txn_hour', 'std_txn_hour', 'mode_txn_hour']\n",
        "\n",
        "    # Spending consistency\n",
        "    daily_spending = transactions.groupby([transactions['Timestamp'].dt.date, 'CustomerID'])['Amount'].sum().reset_index()\n",
        "    spending_consistency = daily_spending.groupby('CustomerID')['Amount'].agg(['mean', 'std']).reset_index()\n",
        "    spending_consistency.columns = ['CustomerID', 'daily_spending_mean', 'daily_spending_std']\n",
        "    spending_consistency['spending_volatility'] = spending_consistency['daily_spending_std'] / (spending_consistency['daily_spending_mean'] + 1)\n",
        "\n",
        "    # Merge all features\n",
        "    features = txn_metrics.merge(channel_behavior, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in mcc_behavior.columns:\n",
        "        features = features.merge(mcc_behavior, on='CustomerID', how='left')\n",
        "    features = features.merge(temporal_patterns, on='CustomerID', how='left')\n",
        "    features = features.merge(spending_consistency, on='CustomerID', how='left')\n",
        "\n",
        "    if 'CustomerID' in large_txn_stats.columns:\n",
        "        features = features.merge(large_txn_stats, on='CustomerID', how='left')\n",
        "\n",
        "    # Risk scores\n",
        "    features['transaction_risk_score'] = (\n",
        "        features.get('large_txn_ratio', 0) * 0.3 +\n",
        "        features['spending_volatility'] * 0.3 +\n",
        "        features['night_ratio'] * 0.2 +\n",
        "        (features['amount_skew'].abs() * 0.2)\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_session_intelligence_features(device_sessions):\n",
        "    \"\"\"Create advanced session behavior intelligence\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "    sessions['Timestamp'] = pd.to_datetime(sessions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Session frequency and patterns\n",
        "    session_freq = sessions.groupby('CustomerID').agg({\n",
        "        'SessionID': 'count',\n",
        "        'City': 'nunique',\n",
        "        'DeviceID': 'nunique',\n",
        "        'IP': 'nunique',\n",
        "        'Timestamp': ['min', 'max']\n",
        "    }).reset_index()\n",
        "    session_freq.columns = ['CustomerID', 'session_count', 'cities_visited', 'devices_used',\n",
        "                           'ips_used', 'first_session', 'last_session']\n",
        "\n",
        "    session_freq['session_period_days'] = (session_freq['last_session'] - session_freq['first_session']).dt.days + 1\n",
        "    session_freq['daily_sessions'] = session_freq['session_count'] / session_freq['session_period_days']\n",
        "\n",
        "    # Advanced action analysis\n",
        "    def analyze_advanced_actions(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'total_actions': len(actions_list),\n",
        "                'financial_actions': 0,\n",
        "                'sensitive_actions': 0,\n",
        "                'login_count': 0,\n",
        "                'transfer_amount': 0,\n",
        "                'payment_amount': 0,\n",
        "                'unique_action_types': set()\n",
        "            }\n",
        "\n",
        "            for action in actions_list:\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', '')\n",
        "                    stats['unique_action_types'].add(action_type)\n",
        "\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                        amount = action.get('amount', 0)\n",
        "                        if action_type == 'transfer':\n",
        "                            stats['transfer_amount'] += amount\n",
        "                        else:\n",
        "                            stats['payment_amount'] += amount\n",
        "                    elif action_type == 'account_view':\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                elif action == 'login':\n",
        "                    stats['login_count'] += 1\n",
        "                    stats['unique_action_types'].add('login')\n",
        "                elif action == 'logout':\n",
        "                    stats['unique_action_types'].add('logout')\n",
        "\n",
        "            stats['unique_action_count'] = len(stats['unique_action_types'])\n",
        "            return stats\n",
        "        return {'total_actions': 0, 'financial_actions': 0, 'sensitive_actions': 0,\n",
        "                'login_count': 0, 'transfer_amount': 0, 'payment_amount': 0, 'unique_action_count': 0}\n",
        "\n",
        "    action_analysis = sessions['Actions'].apply(analyze_advanced_actions)\n",
        "    action_df = pd.DataFrame(action_analysis.tolist())\n",
        "    action_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    # Aggregate action intelligence\n",
        "    action_intel = action_df.groupby('CustomerID').agg({\n",
        "        'total_actions': 'sum',\n",
        "        'financial_actions': 'sum',\n",
        "        'sensitive_actions': 'sum',\n",
        "        'login_count': 'sum',\n",
        "        'transfer_amount': 'sum',\n",
        "        'payment_amount': 'sum',\n",
        "        'unique_action_count': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate behavioral ratios\n",
        "    action_intel['financial_action_ratio'] = action_intel['financial_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['sensitive_action_ratio'] = action_intel['sensitive_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['login_frequency'] = action_intel['login_count'] / session_freq['session_count']\n",
        "    action_intel['avg_financial_amount'] = (action_intel['transfer_amount'] + action_intel['payment_amount']) / (action_intel['financial_actions'] + 1)\n",
        "\n",
        "    # Merge features\n",
        "    features = session_freq.merge(action_intel, on='CustomerID', how='left')\n",
        "\n",
        "    # Security and risk indicators\n",
        "    features['geographic_dispersion'] = features['cities_visited'] / (features['session_count'] + 1)\n",
        "    features['device_diversity'] = features['devices_used'] / (features['session_count'] + 1)\n",
        "    features['ip_diversity'] = features['ips_used'] / (features['session_count'] + 1)\n",
        "\n",
        "    features['session_risk_score'] = (\n",
        "        features['geographic_dispersion'] * 0.3 +\n",
        "        features['device_diversity'] * 0.3 +\n",
        "        features['sensitive_action_ratio'] * 0.4\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "print(\"ðŸ”„ Building elite feature sets...\")\n",
        "\n",
        "# Create all feature sets\n",
        "customer_features = create_elite_customer_features(customers, accounts)\n",
        "temporal_train = create_temporal_dynamics_features(train_panel)\n",
        "temporal_test = create_temporal_dynamics_features(test_panel)\n",
        "\n",
        "# Combine train and test transactions for consistent feature engineering\n",
        "all_transactions = pd.concat([transactions_train, transactions_test])\n",
        "transaction_features = create_advanced_transaction_features(all_transactions)\n",
        "session_features = create_session_intelligence_features(device_sessions)\n",
        "\n",
        "print(\"ðŸŽ¯ Creating final feature matrix...\")\n",
        "\n",
        "def create_elite_feature_matrix(panel_df, customer_df, temporal_df, transaction_df, session_df):\n",
        "    \"\"\"Combine all elite features with intelligent interactions\"\"\"\n",
        "    features = temporal_df.copy()\n",
        "\n",
        "    # Merge all feature sources\n",
        "    features = features.merge(customer_df, on='CustomerID', how='left')\n",
        "    features = features.merge(transaction_df, on='CustomerID', how='left')\n",
        "    features = features.merge(session_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Create powerful interaction features\n",
        "    # Financial stress indicators\n",
        "    stress_components = []\n",
        "    if 'utilization_ma_3' in features.columns:\n",
        "        stress_components.append(features['utilization_ma_3'] * 0.25)\n",
        "    if 'payment_ratio_ma_3' in features.columns:\n",
        "        stress_components.append((1 - features['payment_ratio_ma_3']) * 0.25)\n",
        "    if 'high_risk_profile' in features.columns:\n",
        "        stress_components.append(features['high_risk_profile'] * 0.25)\n",
        "    if 'transaction_risk_score' in features.columns:\n",
        "        stress_components.append(features['transaction_risk_score'] * 0.25)\n",
        "\n",
        "    if stress_components:\n",
        "        features['comprehensive_stress_score'] = sum(stress_components)\n",
        "\n",
        "    # Behavioral risk indicators\n",
        "    behavior_components = []\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        behavior_components.append(features['session_risk_score'] * 0.4)\n",
        "    if 'financial_deterioration' in features.columns:\n",
        "        behavior_components.append(features['financial_deterioration'] * 0.3)\n",
        "    if 'spending_volatility' in features.columns:\n",
        "        behavior_components.append(features['spending_volatility'] * 0.3)\n",
        "\n",
        "    if behavior_components:\n",
        "        features['behavioral_risk_score'] = sum(behavior_components)\n",
        "\n",
        "    # Credit capacity indicators\n",
        "    if 'CreditScore' in features.columns and 'credit_utilization' in features.columns:\n",
        "        features['credit_health_index'] = (\n",
        "            (features['CreditScore'] / 850) * 0.6 +\n",
        "            (1 - features['credit_utilization'].clip(0, 1)) * 0.4\n",
        "        )\n",
        "\n",
        "    # Payment behavior indicators\n",
        "    if 'payment_trend_3' in features.columns and 'payment_volatility' in features.columns:\n",
        "        features['payment_behavior_score'] = (\n",
        "            (1 - features['payment_trend_3'].clip(-1, 0).abs()) * 0.5 +\n",
        "            (1 - features['payment_volatility'].clip(0, 1)) * 0.5\n",
        "        )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "# Create final datasets\n",
        "X_train_elite = create_elite_feature_matrix(train_panel, customer_features, temporal_train, transaction_features, session_features)\n",
        "X_test_elite = create_elite_feature_matrix(test_panel, customer_features, temporal_test, transaction_features, session_features)\n",
        "\n",
        "# Prepare for modeling\n",
        "y_train = X_train_elite['DefaultLabel'].astype(int)\n",
        "non_feature_cols = ['CustomerID', 'Week', 'DefaultLabel', 'first_txn', 'last_txn', 'first_session', 'last_session']\n",
        "feature_cols = [col for col in X_train_elite.columns if col not in non_feature_cols]\n",
        "\n",
        "X_train = X_train_elite[feature_cols]\n",
        "X_test = X_test_elite[feature_cols]\n",
        "\n",
        "print(f\"âœ… Elite feature matrix: {X_train.shape[1]} features, {X_train.shape[0]} samples\")\n",
        "\n",
        "# Ensure numeric types and handle infinite values\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(\"ðŸ” Performing elite feature selection...\")\n",
        "\n",
        "# Use Random Forest for feature selection\n",
        "selector_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "selector_model.fit(X_train, y_train)\n",
        "\n",
        "# Select top features based on importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': selector_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Keep top 50 features\n",
        "top_features = feature_importance.head(50)['feature'].tolist()\n",
        "X_train_selected = X_train[top_features]\n",
        "X_test_selected = X_test[top_features]\n",
        "\n",
        "print(f\"ðŸŽ¯ Selected {len(top_features)} most predictive features\")\n",
        "\n",
        "print(\"ðŸ¤– Training elite ensemble model...\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Train optimized ensemble\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=20,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced_subsample',\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train both models\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Ensemble predictions\n",
        "rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "gb_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Weighted ensemble (favor RF more as it typically performs better)\n",
        "ensemble_proba = rf_proba * 0.7 + gb_proba * 0.3\n",
        "\n",
        "print(\"ðŸ“Š Analyzing optimal threshold...\")\n",
        "\n",
        "# Find optimal threshold for Macro-F1\n",
        "thresholds = np.arange(0.1, 0.5, 0.05)\n",
        "default_rates = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    preds = (ensemble_proba > threshold).astype(int)\n",
        "    default_rates.append(preds.mean())\n",
        "\n",
        "# Select threshold that gives reasonable default rate (5-15%)\n",
        "optimal_threshold = 0.25\n",
        "for i, threshold in enumerate(thresholds):\n",
        "    if 0.05 <= default_rates[i] <= 0.15:\n",
        "        optimal_threshold = threshold\n",
        "        break\n",
        "\n",
        "print(f\"âœ… Optimal threshold: {optimal_threshold:.2f}\")\n",
        "print(f\"   Expected default rate: {default_rates[thresholds.tolist().index(optimal_threshold)]:.3f}\")\n",
        "\n",
        "# Final predictions\n",
        "final_predictions = (ensemble_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'CustomerID': test_panel['CustomerID'],\n",
        "    'Week': test_panel['Week'],\n",
        "    'DefaultLabel': final_predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"retailbanking_challenge2_elite_predictions.csv\", index=False)\n",
        "\n",
        "print(f\"ðŸŽ‰ Elite predictions saved: {len(final_predictions)} predictions\")\n",
        "print(f\"   Default rate: {final_predictions.mean():.3f} ({final_predictions.sum()} defaults)\")\n",
        "\n",
        "# Submit predictions\n",
        "try:\n",
        "    from agentds import BenchmarkClient\n",
        "    client = BenchmarkClient(api_key=\"adsb_E8N9aNAz2w1K7dNYT8BSMGsd_1760199769\", team_name=\"synergy-minds\")\n",
        "\n",
        "    result = client.submit_prediction(\"Retailbanking\", 2, \"retailbanking_challenge2_elite_predictions.csv\")\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"ðŸ† Submission successful!\")\n",
        "        print(f\"   ðŸ“Š Score: {result['score']:.4f}\")\n",
        "        current_score = result['score']\n",
        "        if current_score < 0.9:\n",
        "            improvement_needed = 0.931 - current_score\n",
        "            print(f\"   ðŸŽ¯ Need improvement: {improvement_needed:.4f} to reach top score\")\n",
        "        else:\n",
        "            print(\"   ðŸ’ª Excellent! You're in the top tier!\")\n",
        "    else:\n",
        "        print(\"âŒ Submission failed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ’¥ Submission error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Elite strategy features:\")\n",
        "print(\"   â€¢ Advanced temporal dynamics with trend analysis\")\n",
        "print(\"   â€¢ Sophisticated customer segmentation and risk profiling\")\n",
        "print(\"   â€¢ Comprehensive transaction behavior intelligence\")\n",
        "print(\"   â€¢ Session-based security and risk indicators\")\n",
        "print(\"   â€¢ Ensemble modeling with optimized threshold selection\")\n",
        "print(\"   â€¢ Elite feature selection focusing on predictive power\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbMJ2sP0NS7B",
        "outputId": "a8a5bae2-85a3-45bd-ebf9-8736ea61e238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Loading and preparing data for elite performance...\n",
            "ðŸŽ¯ Creating elite feature engineering pipeline...\n",
            "ðŸ”„ Building elite feature sets...\n",
            "ðŸŽ¯ Creating final feature matrix...\n",
            "âœ… Elite feature matrix: 125 features, 13301 samples\n",
            "ðŸ” Performing elite feature selection...\n",
            "ðŸŽ¯ Selected 50 most predictive features\n",
            "ðŸ¤– Training elite ensemble model...\n",
            "ðŸ“Š Performing temporal cross-validation...\n",
            "  Fold 1: F1 = 0.9420, Threshold = 0.294\n",
            "  Fold 2: F1 = 0.9432, Threshold = 0.254\n",
            "  Fold 3: F1 = 0.9465, Threshold = 0.221\n",
            "  Fold 4: F1 = 0.9476, Threshold = 0.213\n",
            "  Fold 5: F1 = 0.9379, Threshold = 0.322\n",
            "âœ… Cross-validation score: 0.9434 Â± 0.0034\n",
            "ðŸŽ¯ Training final ensemble models...\n",
            "ðŸ“Š Analyzing optimal threshold...\n",
            "âœ… Optimal threshold: 0.452\n",
            "   Expected F1 score: 0.9989\n",
            "ðŸŽ‰ Elite predictions saved: 13290 predictions\n",
            "   Default rate: 0.094 (1249 defaults)\n",
            "ðŸ” Top predictive features:\n",
            "                   feature          category  importance\n",
            "0      unique_action_count             Other    0.187503\n",
            "3              CreditScore       Credit Risk    0.099288\n",
            "5      credit_health_index             Other    0.082285\n",
            "6     credit_behavior_risk             Other    0.072777\n",
            "1       low_payment_streak             Other    0.069266\n",
            "4            total_actions             Other    0.064282\n",
            "2            HardInquiries             Other    0.058443\n",
            "8   sensitive_action_ratio  Session Behavior    0.034419\n",
            "7       session_risk_score  Session Behavior    0.028737\n",
            "25              amount_std             Other    0.017109\n",
            "âœ… Prediction submitted successfully!\n",
            "ðŸ“Š Score: 0.9438 (Macro-F1)\n",
            "âœ… Validation passed\n",
            "ðŸ† Submission successful!\n",
            "   ðŸ“Š Score: 0.9438\n",
            "   ðŸ’ª Excellent! You're in the top tier!\n",
            "\n",
            "ðŸ’¡ Elite strategy features:\n",
            "   â€¢ Advanced temporal dynamics with trend analysis\n",
            "   â€¢ Sophisticated customer segmentation and risk profiling\n",
            "   â€¢ Comprehensive transaction behavior intelligence\n",
            "   â€¢ Session-based security and risk indicators\n",
            "   â€¢ Ensemble modeling with optimized threshold selection\n",
            "   â€¢ Elite feature selection focusing on predictive power\n",
            "   â€¢ Business-constrained threshold optimization\n",
            "   â€¢ Temporal cross-validation for robust performance\n",
            "   â€¢ Feature interaction engineering\n",
            "   â€¢ Model interpretation and business insights\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "    print(\"XGBoost not available, using alternative models\")\n",
        "\n",
        "print(\"ðŸš€ Loading and preparing data for elite performance...\")\n",
        "\n",
        "# Load data\n",
        "customers = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customers_all.csv\")\n",
        "accounts = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/accounts_all.csv\")\n",
        "transactions_train = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_train.csv\")\n",
        "transactions_test = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_test.csv\")\n",
        "train_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_train.csv\")\n",
        "test_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_test.csv\")\n",
        "\n",
        "with open(\"/home/jovyan/shared/datasets/RetailBanking/device_sessions_all.json\", 'r') as f:\n",
        "    import json\n",
        "    device_sessions = pd.json_normalize(json.load(f))\n",
        "\n",
        "print(\"ðŸŽ¯ Creating elite feature engineering pipeline...\")\n",
        "\n",
        "def create_elite_customer_features(customers, accounts):\n",
        "    \"\"\"Create sophisticated customer segmentation and risk profiling\"\"\"\n",
        "    features = customers.copy()\n",
        "\n",
        "    # Advanced encoding\n",
        "    le_city = LabelEncoder()\n",
        "    features['HomeCity_encoded'] = le_city.fit_transform(features['HomeCity'].fillna('Unknown'))\n",
        "\n",
        "    # Account portfolio analysis\n",
        "    account_metrics = accounts.groupby('CustomerID').agg({\n",
        "        'AccountID': 'count',\n",
        "        'Balance': ['sum', 'mean', 'std', 'max', 'min', 'median'],\n",
        "        'Limit': ['sum', 'max', 'mean'],\n",
        "        'Type': lambda x: x.nunique()\n",
        "    }).reset_index()\n",
        "    account_metrics.columns = ['CustomerID', 'total_accounts', 'balance_sum', 'balance_mean',\n",
        "                              'balance_std', 'balance_max', 'balance_min', 'balance_median',\n",
        "                              'limit_sum', 'limit_max', 'limit_mean', 'account_type_diversity']\n",
        "\n",
        "    # Credit-specific features\n",
        "    credit_accounts = accounts[accounts['Type'] == 'credit_card']\n",
        "    if not credit_accounts.empty:\n",
        "        credit_metrics = credit_accounts.groupby('CustomerID').agg({\n",
        "            'Balance': ['sum', 'mean', 'max'],\n",
        "            'Limit': ['sum', 'max', 'mean']\n",
        "        }).reset_index()\n",
        "        credit_metrics.columns = ['CustomerID', 'credit_balance_sum', 'credit_balance_mean',\n",
        "                                 'credit_balance_max', 'credit_limit_sum', 'credit_limit_max',\n",
        "                                 'credit_limit_mean']\n",
        "\n",
        "        credit_metrics['credit_utilization'] = credit_metrics['credit_balance_sum'] / (credit_metrics['credit_limit_sum'] + 1)\n",
        "        credit_metrics['max_credit_utilization'] = credit_metrics['credit_balance_max'] / (credit_metrics['credit_limit_max'] + 1)\n",
        "    else:\n",
        "        credit_metrics = pd.DataFrame(columns=['CustomerID', 'credit_utilization', 'max_credit_utilization'])\n",
        "\n",
        "    # Account type composition\n",
        "    account_composition = pd.get_dummies(accounts[['CustomerID', 'Type']], columns=['Type'], prefix='account')\n",
        "    account_composition = account_composition.groupby('CustomerID').sum().reset_index()\n",
        "\n",
        "    # Merge features\n",
        "    features = features.merge(account_metrics, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in credit_metrics.columns:\n",
        "        features = features.merge(credit_metrics, on='CustomerID', how='left')\n",
        "    features = features.merge(account_composition, on='CustomerID', how='left')\n",
        "\n",
        "    # Advanced financial ratios\n",
        "    features['balance_to_salary'] = features['balance_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['limit_to_salary'] = features['limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['credit_depth'] = features['credit_limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "\n",
        "    # Risk profiling\n",
        "    features['high_risk_profile'] = (\n",
        "        (features['CreditScore'] < 580).astype(int) * 0.4 +\n",
        "        (features.get('credit_utilization', 0) > 0.8).astype(int) * 0.3 +\n",
        "        (features['balance_to_salary'] > 0.5).astype(int) * 0.3\n",
        "    )\n",
        "\n",
        "    # Customer lifetime value proxy\n",
        "    features['clv_score'] = (\n",
        "        (features['AnnualSalary'] / features['AnnualSalary'].max()) * 0.4 +\n",
        "        (features['CreditScore'] / 850) * 0.3 +\n",
        "        (features['Tenure'] / features['Tenure'].max()) * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def enhance_temporal_features(panel_df):\n",
        "    \"\"\"Add more sophisticated time-series features\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Seasonal decomposition\n",
        "    features['utilization_seasonal'] = features.groupby('CustomerID')['Utilisation'].transform(\n",
        "        lambda x: x - x.rolling(4, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "    # Change point detection\n",
        "    features['utilization_change_point'] = (\n",
        "        features['Utilisation'].rolling(3).std() > features['Utilisation'].rolling(6).std() * 1.5\n",
        "    ).astype(int)\n",
        "\n",
        "    # Momentum indicators\n",
        "    features['utilization_momentum'] = features['Utilisation'] - features['Utilisation'].shift(2)\n",
        "    features['payment_momentum'] = features['PaymentRatio'] - features['PaymentRatio'].shift(2)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_temporal_dynamics_features(panel_df):\n",
        "    \"\"\"Create sophisticated time-series features with trend analysis\"\"\"\n",
        "    temporal_features = []\n",
        "\n",
        "    for customer_id in panel_df['CustomerID'].unique():\n",
        "        cust_data = panel_df[panel_df['CustomerID'] == customer_id].sort_values('Week')\n",
        "\n",
        "        # Rolling statistics with multiple windows\n",
        "        for window in [2, 3, 4]:\n",
        "            cust_data[f'utilization_ma_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'payment_ratio_ma_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'utilization_std_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).std()\n",
        "            cust_data[f'payment_ratio_std_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).std()\n",
        "\n",
        "        # Trend analysis\n",
        "        cust_data['utilization_trend_3'] = cust_data['Utilisation'].diff(periods=2).fillna(0)\n",
        "        cust_data['payment_trend_3'] = cust_data['PaymentRatio'].diff(periods=2).fillna(0)\n",
        "\n",
        "        # Acceleration (second derivative)\n",
        "        cust_data['utilization_acceleration'] = cust_data['utilization_trend_3'].diff().fillna(0)\n",
        "        cust_data['payment_acceleration'] = cust_data['payment_trend_3'].diff().fillna(0)\n",
        "\n",
        "        # Volatility measures\n",
        "        cust_data['utilization_volatility'] = cust_data['Utilisation'].rolling(4, min_periods=1).std()\n",
        "        cust_data['payment_volatility'] = cust_data['PaymentRatio'].rolling(4, min_periods=1).std()\n",
        "\n",
        "        # Behavioral patterns\n",
        "        cust_data['high_utilization_streak'] = (cust_data['Utilisation'] > 0.7).astype(int)\n",
        "        cust_data['low_payment_streak'] = (cust_data['PaymentRatio'] < 0.2).astype(int)\n",
        "\n",
        "        # Calculate streaks\n",
        "        for i in range(1, len(cust_data)):\n",
        "            if cust_data.iloc[i]['high_utilization_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('high_utilization_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['high_utilization_streak'] + 1\n",
        "            if cust_data.iloc[i]['low_payment_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('low_payment_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['low_payment_streak'] + 1\n",
        "\n",
        "        # Deterioration indicators\n",
        "        cust_data['financial_deterioration'] = (\n",
        "            (cust_data['utilization_trend_3'] > 0).astype(int) * 0.5 +\n",
        "            (cust_data['payment_trend_3'] < 0).astype(int) * 0.5\n",
        "        )\n",
        "\n",
        "        temporal_features.append(cust_data)\n",
        "\n",
        "    result = pd.concat(temporal_features, ignore_index=True).fillna(0)\n",
        "\n",
        "    # Apply enhanced temporal features\n",
        "    result = enhance_temporal_features(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_advanced_transaction_features(transactions_df):\n",
        "    \"\"\"Create comprehensive transaction behavior profiling\"\"\"\n",
        "    transactions = transactions_df.copy()\n",
        "    transactions['Timestamp'] = pd.to_datetime(transactions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Basic transaction metrics\n",
        "    txn_metrics = transactions.groupby('CustomerID').agg({\n",
        "        'TxnID': 'count',\n",
        "        'Amount': ['sum', 'mean', 'std', 'max', 'min', 'median', 'skew'],\n",
        "        'Timestamp': ['min', 'max', 'nunique']\n",
        "    }).reset_index()\n",
        "    txn_metrics.columns = ['CustomerID', 'txn_count', 'amount_sum', 'amount_mean', 'amount_std',\n",
        "                          'amount_max', 'amount_min', 'amount_median', 'amount_skew',\n",
        "                          'first_txn', 'last_txn', 'active_days']\n",
        "\n",
        "    # Transaction period and velocity\n",
        "    txn_metrics['txn_period_days'] = (txn_metrics['last_txn'] - txn_metrics['first_txn']).dt.days + 1\n",
        "    txn_metrics['daily_txn_frequency'] = txn_metrics['txn_count'] / txn_metrics['txn_period_days']\n",
        "    txn_metrics['daily_spending'] = txn_metrics['amount_sum'] / txn_metrics['txn_period_days']\n",
        "\n",
        "    # Large transaction analysis\n",
        "    large_txn_threshold = transactions['Amount'].quantile(0.8)\n",
        "    large_txns = transactions[transactions['Amount'] > large_txn_threshold]\n",
        "\n",
        "    if not large_txns.empty:\n",
        "        large_txn_stats = large_txns.groupby('CustomerID').agg({\n",
        "            'TxnID': 'count',\n",
        "            'Amount': ['mean', 'sum', 'max']\n",
        "        }).reset_index()\n",
        "        large_txn_stats.columns = ['CustomerID', 'large_txn_count', 'large_txn_avg',\n",
        "                                  'large_txn_sum', 'large_txn_max']\n",
        "\n",
        "        large_txn_stats['large_txn_ratio'] = large_txn_stats['large_txn_count'] / txn_metrics['txn_count']\n",
        "        large_txn_stats['large_amount_ratio'] = large_txn_stats['large_txn_sum'] / txn_metrics['amount_sum']\n",
        "    else:\n",
        "        large_txn_stats = pd.DataFrame(columns=['CustomerID', 'large_txn_ratio', 'large_amount_ratio'])\n",
        "\n",
        "    # Channel behavior\n",
        "    channel_behavior = pd.get_dummies(transactions[['CustomerID', 'Channel']],\n",
        "                                    columns=['Channel'], prefix='channel')\n",
        "    channel_behavior = channel_behavior.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "    # MCC spending patterns\n",
        "    if 'MCC_Group' in transactions.columns:\n",
        "        mcc_behavior = pd.get_dummies(transactions[['CustomerID', 'MCC_Group']],\n",
        "                                    columns=['MCC_Group'], prefix='mcc')\n",
        "        mcc_behavior = mcc_behavior.groupby('CustomerID').mean().reset_index()\n",
        "    else:\n",
        "        mcc_behavior = pd.DataFrame(columns=['CustomerID'])\n",
        "\n",
        "    # Temporal patterns\n",
        "    transactions['hour'] = transactions['Timestamp'].dt.hour\n",
        "    transactions['day_of_week'] = transactions['Timestamp'].dt.dayofweek\n",
        "    transactions['is_weekend'] = (transactions['day_of_week'] >= 5).astype(int)\n",
        "    transactions['is_night'] = ((transactions['hour'] >= 22) | (transactions['hour'] <= 6)).astype(int)\n",
        "\n",
        "    temporal_patterns = transactions.groupby('CustomerID').agg({\n",
        "        'is_weekend': 'mean',\n",
        "        'is_night': 'mean',\n",
        "        'hour': ['mean', 'std', lambda x: x.mode()[0] if len(x.mode()) > 0 else 12]\n",
        "    }).reset_index()\n",
        "    temporal_patterns.columns = ['CustomerID', 'weekend_ratio', 'night_ratio',\n",
        "                                'avg_txn_hour', 'std_txn_hour', 'mode_txn_hour']\n",
        "\n",
        "    # Spending consistency\n",
        "    daily_spending = transactions.groupby([transactions['Timestamp'].dt.date, 'CustomerID'])['Amount'].sum().reset_index()\n",
        "    spending_consistency = daily_spending.groupby('CustomerID')['Amount'].agg(['mean', 'std']).reset_index()\n",
        "    spending_consistency.columns = ['CustomerID', 'daily_spending_mean', 'daily_spending_std']\n",
        "    spending_consistency['spending_volatility'] = spending_consistency['daily_spending_std'] / (spending_consistency['daily_spending_mean'] + 1)\n",
        "\n",
        "    # Merge all features\n",
        "    features = txn_metrics.merge(channel_behavior, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in mcc_behavior.columns:\n",
        "        features = features.merge(mcc_behavior, on='CustomerID', how='left')\n",
        "    features = features.merge(temporal_patterns, on='CustomerID', how='left')\n",
        "    features = features.merge(spending_consistency, on='CustomerID', how='left')\n",
        "\n",
        "    if 'CustomerID' in large_txn_stats.columns:\n",
        "        features = features.merge(large_txn_stats, on='CustomerID', how='left')\n",
        "\n",
        "    # Risk scores\n",
        "    features['transaction_risk_score'] = (\n",
        "        features.get('large_txn_ratio', 0) * 0.3 +\n",
        "        features['spending_volatility'] * 0.3 +\n",
        "        features['night_ratio'] * 0.2 +\n",
        "        (features['amount_skew'].abs() * 0.2)\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_session_intelligence_features(device_sessions):\n",
        "    \"\"\"Create advanced session behavior intelligence\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "    sessions['Timestamp'] = pd.to_datetime(sessions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Session frequency and patterns\n",
        "    session_freq = sessions.groupby('CustomerID').agg({\n",
        "        'SessionID': 'count',\n",
        "        'City': 'nunique',\n",
        "        'DeviceID': 'nunique',\n",
        "        'IP': 'nunique',\n",
        "        'Timestamp': ['min', 'max']\n",
        "    }).reset_index()\n",
        "    session_freq.columns = ['CustomerID', 'session_count', 'cities_visited', 'devices_used',\n",
        "                           'ips_used', 'first_session', 'last_session']\n",
        "\n",
        "    session_freq['session_period_days'] = (session_freq['last_session'] - session_freq['first_session']).dt.days + 1\n",
        "    session_freq['daily_sessions'] = session_freq['session_count'] / session_freq['session_period_days']\n",
        "\n",
        "    # Advanced action analysis\n",
        "    def analyze_advanced_actions(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'total_actions': len(actions_list),\n",
        "                'financial_actions': 0,\n",
        "                'sensitive_actions': 0,\n",
        "                'login_count': 0,\n",
        "                'transfer_amount': 0,\n",
        "                'payment_amount': 0,\n",
        "                'unique_action_types': set()\n",
        "            }\n",
        "\n",
        "            for action in actions_list:\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', '')\n",
        "                    stats['unique_action_types'].add(action_type)\n",
        "\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                        amount = action.get('amount', 0)\n",
        "                        if action_type == 'transfer':\n",
        "                            stats['transfer_amount'] += amount\n",
        "                        else:\n",
        "                            stats['payment_amount'] += amount\n",
        "                    elif action_type == 'account_view':\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                elif action == 'login':\n",
        "                    stats['login_count'] += 1\n",
        "                    stats['unique_action_types'].add('login')\n",
        "                elif action == 'logout':\n",
        "                    stats['unique_action_types'].add('logout')\n",
        "\n",
        "            stats['unique_action_count'] = len(stats['unique_action_types'])\n",
        "            return stats\n",
        "        return {'total_actions': 0, 'financial_actions': 0, 'sensitive_actions': 0,\n",
        "                'login_count': 0, 'transfer_amount': 0, 'payment_amount': 0, 'unique_action_count': 0}\n",
        "\n",
        "    action_analysis = sessions['Actions'].apply(analyze_advanced_actions)\n",
        "    action_df = pd.DataFrame(action_analysis.tolist())\n",
        "    action_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    # Aggregate action intelligence\n",
        "    action_intel = action_df.groupby('CustomerID').agg({\n",
        "        'total_actions': 'sum',\n",
        "        'financial_actions': 'sum',\n",
        "        'sensitive_actions': 'sum',\n",
        "        'login_count': 'sum',\n",
        "        'transfer_amount': 'sum',\n",
        "        'payment_amount': 'sum',\n",
        "        'unique_action_count': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate behavioral ratios\n",
        "    action_intel['financial_action_ratio'] = action_intel['financial_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['sensitive_action_ratio'] = action_intel['sensitive_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['login_frequency'] = action_intel['login_count'] / session_freq['session_count']\n",
        "    action_intel['avg_financial_amount'] = (action_intel['transfer_amount'] + action_intel['payment_amount']) / (action_intel['financial_actions'] + 1)\n",
        "\n",
        "    # Merge features\n",
        "    features = session_freq.merge(action_intel, on='CustomerID', how='left')\n",
        "\n",
        "    # Security and risk indicators\n",
        "    features['geographic_dispersion'] = features['cities_visited'] / (features['session_count'] + 1)\n",
        "    features['device_diversity'] = features['devices_used'] / (features['session_count'] + 1)\n",
        "    features['ip_diversity'] = features['ips_used'] / (features['session_count'] + 1)\n",
        "\n",
        "    features['session_risk_score'] = (\n",
        "        features['geographic_dispersion'] * 0.3 +\n",
        "        features['device_diversity'] * 0.3 +\n",
        "        features['sensitive_action_ratio'] * 0.4\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_interaction_features(features):\n",
        "    \"\"\"Create powerful interaction terms\"\"\"\n",
        "    # Financial stress interactions\n",
        "    if all(col in features.columns for col in ['utilization_ma_3', 'payment_ratio_ma_3']):\n",
        "        features['utilization_payment_interaction'] = (\n",
        "            features['utilization_ma_3'] * (1 - features['payment_ratio_ma_3'])\n",
        "        )\n",
        "\n",
        "    # Credit-behavior interactions\n",
        "    if all(col in features.columns for col in ['CreditScore', 'transaction_risk_score']):\n",
        "        features['credit_behavior_risk'] = (\n",
        "            (1 - features['CreditScore'] / 850) * features['transaction_risk_score']\n",
        "        )\n",
        "\n",
        "    # Multi-dimensional risk scoring\n",
        "    risk_components = []\n",
        "    risk_cols = ['high_risk_profile', 'transaction_risk_score', 'session_risk_score', 'financial_deterioration']\n",
        "\n",
        "    for col in risk_cols:\n",
        "        if col in features.columns:\n",
        "            risk_components.append(features[col])\n",
        "\n",
        "    if risk_components:\n",
        "        features['comprehensive_risk_index'] = sum(risk_components) / len(risk_components)\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"ðŸ”„ Building elite feature sets...\")\n",
        "\n",
        "# Create all feature sets\n",
        "customer_features = create_elite_customer_features(customers, accounts)\n",
        "temporal_train = create_temporal_dynamics_features(train_panel)\n",
        "temporal_test = create_temporal_dynamics_features(test_panel)\n",
        "\n",
        "# Combine train and test transactions for consistent feature engineering\n",
        "all_transactions = pd.concat([transactions_train, transactions_test])\n",
        "transaction_features = create_advanced_transaction_features(all_transactions)\n",
        "session_features = create_session_intelligence_features(device_sessions)\n",
        "\n",
        "print(\"ðŸŽ¯ Creating final feature matrix...\")\n",
        "\n",
        "def create_elite_feature_matrix(panel_df, customer_df, temporal_df, transaction_df, session_df):\n",
        "    \"\"\"Combine all elite features with intelligent interactions\"\"\"\n",
        "    features = temporal_df.copy()\n",
        "\n",
        "    # Merge all feature sources\n",
        "    features = features.merge(customer_df, on='CustomerID', how='left')\n",
        "    features = features.merge(transaction_df, on='CustomerID', how='left')\n",
        "    features = features.merge(session_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Create powerful interaction features\n",
        "    # Financial stress indicators\n",
        "    stress_components = []\n",
        "    if 'utilization_ma_3' in features.columns:\n",
        "        stress_components.append(features['utilization_ma_3'] * 0.25)\n",
        "    if 'payment_ratio_ma_3' in features.columns:\n",
        "        stress_components.append((1 - features['payment_ratio_ma_3']) * 0.25)\n",
        "    if 'high_risk_profile' in features.columns:\n",
        "        stress_components.append(features['high_risk_profile'] * 0.25)\n",
        "    if 'transaction_risk_score' in features.columns:\n",
        "        stress_components.append(features['transaction_risk_score'] * 0.25)\n",
        "\n",
        "    if stress_components:\n",
        "        features['comprehensive_stress_score'] = sum(stress_components)\n",
        "\n",
        "    # Behavioral risk indicators\n",
        "    behavior_components = []\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        behavior_components.append(features['session_risk_score'] * 0.4)\n",
        "    if 'financial_deterioration' in features.columns:\n",
        "        behavior_components.append(features['financial_deterioration'] * 0.3)\n",
        "    if 'spending_volatility' in features.columns:\n",
        "        behavior_components.append(features['spending_volatility'] * 0.3)\n",
        "\n",
        "    if behavior_components:\n",
        "        features['behavioral_risk_score'] = sum(behavior_components)\n",
        "\n",
        "    # Credit capacity indicators\n",
        "    if 'CreditScore' in features.columns and 'credit_utilization' in features.columns:\n",
        "        features['credit_health_index'] = (\n",
        "            (features['CreditScore'] / 850) * 0.6 +\n",
        "            (1 - features['credit_utilization'].clip(0, 1)) * 0.4\n",
        "        )\n",
        "\n",
        "    # Payment behavior indicators\n",
        "    if 'payment_trend_3' in features.columns and 'payment_volatility' in features.columns:\n",
        "        features['payment_behavior_score'] = (\n",
        "            (1 - features['payment_trend_3'].clip(-1, 0).abs()) * 0.5 +\n",
        "            (1 - features['payment_volatility'].clip(0, 1)) * 0.5\n",
        "        )\n",
        "\n",
        "    # Apply interaction features\n",
        "    features = create_interaction_features(features)\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "# Create final datasets\n",
        "X_train_elite = create_elite_feature_matrix(train_panel, customer_features, temporal_train, transaction_features, session_features)\n",
        "X_test_elite = create_elite_feature_matrix(test_panel, customer_features, temporal_test, transaction_features, session_features)\n",
        "\n",
        "# Prepare for modeling\n",
        "y_train = X_train_elite['DefaultLabel'].astype(int)\n",
        "non_feature_cols = ['CustomerID', 'Week', 'DefaultLabel', 'first_txn', 'last_txn', 'first_session', 'last_session']\n",
        "feature_cols = [col for col in X_train_elite.columns if col not in non_feature_cols]\n",
        "\n",
        "X_train = X_train_elite[feature_cols]\n",
        "X_test = X_test_elite[feature_cols]\n",
        "\n",
        "print(f\"âœ… Elite feature matrix: {X_train.shape[1]} features, {X_train.shape[0]} samples\")\n",
        "\n",
        "# Ensure numeric types and handle infinite values\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(\"ðŸ” Performing elite feature selection...\")\n",
        "\n",
        "# Use Random Forest for feature selection\n",
        "selector_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "selector_model.fit(X_train, y_train)\n",
        "\n",
        "# Select top features based on importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': selector_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Keep top 50 features\n",
        "top_features = feature_importance.head(50)['feature'].tolist()\n",
        "X_train_selected = X_train[top_features]\n",
        "X_test_selected = X_test[top_features]\n",
        "\n",
        "print(f\"ðŸŽ¯ Selected {len(top_features)} most predictive features\")\n",
        "\n",
        "def optimize_threshold_with_business_rules(y_true, y_proba, min_default_rate=0.05, max_default_rate=0.15):\n",
        "    \"\"\"Optimize threshold considering business constraints\"\"\"\n",
        "    thresholds = np.linspace(0.1, 0.5, 100)\n",
        "    best_threshold = 0.25\n",
        "    best_f1 = 0\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_proba > threshold).astype(int)\n",
        "        default_rate = y_pred.mean()\n",
        "\n",
        "        # Apply business constraints\n",
        "        if min_default_rate <= default_rate <= max_default_rate:\n",
        "            f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_threshold, best_f1\n",
        "\n",
        "def create_stacked_ensemble(X_train, y_train, X_test):\n",
        "    \"\"\"Create stacked ensemble with multiple base models\"\"\"\n",
        "    base_models = {\n",
        "        'rf': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "        'gb': GradientBoostingClassifier(n_estimators=150, random_state=42)\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGB_AVAILABLE:\n",
        "        base_models['xgb'] = xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Create meta-features using cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    meta_train = np.zeros((X_train.shape[0], len(base_models)))\n",
        "    meta_test = np.zeros((X_test.shape[0], len(base_models)))\n",
        "\n",
        "    for i, (name, model) in enumerate(base_models.items()):\n",
        "        fold_predictions = np.zeros(X_train.shape[0])\n",
        "        test_predictions = []\n",
        "\n",
        "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "            model.fit(X_train[train_idx], y_train[train_idx])\n",
        "            fold_predictions[val_idx] = model.predict_proba(X_train[val_idx])[:, 1]\n",
        "            test_predictions.append(model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "        meta_train[:, i] = fold_predictions\n",
        "        meta_test[:, i] = np.mean(test_predictions, axis=0)\n",
        "\n",
        "    # Meta-model\n",
        "    meta_model = LogisticRegression()\n",
        "    meta_model.fit(meta_train, y_train)\n",
        "\n",
        "    return meta_model.predict_proba(meta_test)[:, 1]\n",
        "\n",
        "def temporal_cross_validation(model, X, y, weeks, n_splits=5):\n",
        "    \"\"\"Time-aware cross-validation\"\"\"\n",
        "    unique_weeks = sorted(weeks.unique())\n",
        "    split_size = len(unique_weeks) // n_splits\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        val_weeks = unique_weeks[i * split_size:(i + 1) * split_size]\n",
        "\n",
        "        train_mask = ~weeks.isin(val_weeks)\n",
        "        val_mask = weeks.isin(val_weeks)\n",
        "\n",
        "        X_train_fold, X_val = X[train_mask], X[val_mask]\n",
        "        y_train_fold, y_val = y[train_mask], y[val_mask]\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        y_pred = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # Optimize threshold for this fold\n",
        "        threshold, _ = optimize_threshold_with_business_rules(y_val, y_pred)\n",
        "        val_pred = (y_pred > threshold).astype(int)\n",
        "\n",
        "        score = f1_score(y_val, val_pred, average='macro')\n",
        "        scores.append(score)\n",
        "        print(f\"  Fold {i+1}: F1 = {score:.4f}, Threshold = {threshold:.3f}\")\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "print(\"ðŸ¤– Training elite ensemble model...\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Perform temporal cross-validation\n",
        "print(\"ðŸ“Š Performing temporal cross-validation...\")\n",
        "rf_cv_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "cv_score, cv_std = temporal_cross_validation(rf_cv_model, X_train_scaled, y_train, X_train_elite['Week'])\n",
        "print(f\"âœ… Cross-validation score: {cv_score:.4f} Â± {cv_std:.4f}\")\n",
        "\n",
        "# Train optimized ensemble\n",
        "print(\"ðŸŽ¯ Training final ensemble models...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=20,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced_subsample',\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train both models\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Option 1: Simple weighted ensemble\n",
        "rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "gb_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "ensemble_proba = rf_proba * 0.7 + gb_proba * 0.3\n",
        "\n",
        "# Option 2: Stacked ensemble (uncomment to use)\n",
        "# print(\"ðŸ”„ Training stacked ensemble...\")\n",
        "# stacked_proba = create_stacked_ensemble(X_train_scaled, y_train, X_test_scaled)\n",
        "# ensemble_proba = stacked_proba  # Use stacked ensemble instead\n",
        "\n",
        "print(\"ðŸ“Š Analyzing optimal threshold...\")\n",
        "\n",
        "# Find optimal threshold for Macro-F1 with business constraints\n",
        "optimal_threshold, optimal_f1 = optimize_threshold_with_business_rules(\n",
        "    y_train,\n",
        "    rf_model.predict_proba(X_train_scaled)[:, 1]  # Use RF for threshold optimization\n",
        ")\n",
        "\n",
        "print(f\"âœ… Optimal threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"   Expected F1 score: {optimal_f1:.4f}\")\n",
        "\n",
        "# Final predictions\n",
        "final_predictions = (ensemble_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'CustomerID': test_panel['CustomerID'],\n",
        "    'Week': test_panel['Week'],\n",
        "    'DefaultLabel': final_predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"retailbanking_challenge2_elite_predictions.csv\", index=False)\n",
        "\n",
        "print(f\"ðŸŽ‰ Elite predictions saved: {len(final_predictions)} predictions\")\n",
        "print(f\"   Default rate: {final_predictions.mean():.3f} ({final_predictions.sum()} defaults)\")\n",
        "\n",
        "# Model interpretation\n",
        "def explain_predictions(model, feature_names, top_n=20):\n",
        "    \"\"\"Provide business-interpretable feature importance\"\"\"\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(top_n)\n",
        "\n",
        "    # Categorize features for business interpretation\n",
        "    categories = {\n",
        "        'Credit Risk': ['CreditScore', 'credit_utilization', 'high_risk_profile'],\n",
        "        'Payment Behavior': ['PaymentRatio', 'payment_trend', 'payment_volatility'],\n",
        "        'Spending Patterns': ['utilization_ma', 'transaction_risk_score', 'spending_volatility'],\n",
        "        'Session Behavior': ['session_risk_score', 'financial_actions', 'sensitive_action_ratio']\n",
        "    }\n",
        "\n",
        "    for feature in importance_df['feature']:\n",
        "        for category, keywords in categories.items():\n",
        "            if any(keyword in feature for keyword in keywords):\n",
        "                importance_df.loc[importance_df['feature'] == feature, 'category'] = category\n",
        "                break\n",
        "        else:\n",
        "            importance_df.loc[importance_df['feature'] == feature, 'category'] = 'Other'\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "print(\"ðŸ” Top predictive features:\")\n",
        "feature_explanation = explain_predictions(rf_model, top_features)\n",
        "print(feature_explanation[['feature', 'category', 'importance']].head(10))\n",
        "\n",
        "# Submit predictions\n",
        "try:\n",
        "    from agentds import BenchmarkClient\n",
        "    client = BenchmarkClient(api_key=\"adsb_E8N9aNAz2w1K7dNYT8BSMGsd_1760199769\", team_name=\"synergy-minds\")\n",
        "\n",
        "    result = client.submit_prediction(\"Retailbanking\", 2, \"retailbanking_challenge2_elite_predictions.csv\")\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"ðŸ† Submission successful!\")\n",
        "        print(f\"   ðŸ“Š Score: {result['score']:.4f}\")\n",
        "        current_score = result['score']\n",
        "        if current_score < 0.9:\n",
        "            improvement_needed = 0.931 - current_score\n",
        "            print(f\"   ðŸŽ¯ Need improvement: {improvement_needed:.4f} to reach top score\")\n",
        "        else:\n",
        "            print(\"   ðŸ’ª Excellent! You're in the top tier!\")\n",
        "    else:\n",
        "        print(\"âŒ Submission failed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ’¥ Submission error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Elite strategy features:\")\n",
        "print(\"   â€¢ Advanced temporal dynamics with trend analysis\")\n",
        "print(\"   â€¢ Sophisticated customer segmentation and risk profiling\")\n",
        "print(\"   â€¢ Comprehensive transaction behavior intelligence\")\n",
        "print(\"   â€¢ Session-based security and risk indicators\")\n",
        "print(\"   â€¢ Ensemble modeling with optimized threshold selection\")\n",
        "print(\"   â€¢ Elite feature selection focusing on predictive power\")\n",
        "print(\"   â€¢ Business-constrained threshold optimization\")\n",
        "print(\"   â€¢ Temporal cross-validation for robust performance\")\n",
        "print(\"   â€¢ Feature interaction engineering\")\n",
        "print(\"   â€¢ Model interpretation and business insights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZJDxKV2NS7C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MunJquLNS7C",
        "outputId": "2fb74dec-c702-4b8b-8ab2-6b7edd555381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Loading and preparing data for elite performance...\n",
            "ðŸŽ¯ Creating elite feature engineering pipeline...\n",
            "ðŸ”„ Building elite feature sets...\n",
            "ðŸŽ¯ Creating final feature matrix...\n",
            "âœ… Elite feature matrix: 143 features, 13301 samples\n",
            "ðŸ” Performing elite feature selection...\n",
            "ðŸŽ¯ Selected 60 most predictive features\n",
            "ðŸ¤– Training elite ensemble model...\n",
            "ðŸ“Š Performing temporal cross-validation...\n",
            "  Fold 1: F1 = 0.9425, Threshold = 0.294\n",
            "  Fold 2: F1 = 0.9442, Threshold = 0.254\n",
            "  Fold 3: F1 = 0.9455, Threshold = 0.241\n",
            "  Fold 4: F1 = 0.9478, Threshold = 0.193\n",
            "  Fold 5: F1 = 0.9388, Threshold = 0.330\n",
            "âœ… Cross-validation score: 0.9438 Â± 0.0030\n",
            "âœ… Average optimal threshold: 0.262\n",
            "ðŸŽ¯ Training final ensemble models...\n",
            "ðŸ“Š Analyzing optimal threshold...\n",
            "âœ… Optimal threshold: 0.431\n",
            "   Expected F1 score: 1.0000\n",
            "ðŸŽ‰ Elite enhanced predictions saved: 13290 predictions\n",
            "   Default rate: 0.099 (1311 defaults)\n",
            "ðŸ” Top predictive features:\n",
            "                       feature          category  importance\n",
            "0   action_sequence_complexity  Session Behavior    0.137472\n",
            "1          unique_action_count             Other    0.124731\n",
            "4   session_credit_interaction             Other    0.086630\n",
            "5                  CreditScore       Credit Risk    0.074455\n",
            "2           low_payment_streak             Other    0.064084\n",
            "6          credit_health_index       Credit Risk    0.063403\n",
            "3                HardInquiries             Other    0.055431\n",
            "7         credit_behavior_risk             Other    0.050951\n",
            "9                total_actions             Other    0.045256\n",
            "8       sensitive_action_ratio  Session Behavior    0.022378\n",
            "11          session_risk_score  Session Behavior    0.021424\n",
            "54  comprehensive_stress_score  Composite Scores    0.014061\n",
            "23                   limit_max             Other    0.013420\n",
            "28                  amount_std             Other    0.012138\n",
            "48         credit_limit_mean_x             Other    0.012071\n",
            "âœ… Prediction submitted successfully!\n",
            "ðŸ“Š Score: 0.9506 (Macro-F1)\n",
            "âœ… Validation passed\n",
            "ðŸ† Submission successful!\n",
            "   ðŸ“Š Score: 0.9506\n",
            "   ðŸ’ª Excellent! You're in the elite tier!\n",
            "   ðŸ“ˆ Improvement from previous: +0.0068\n",
            "\n",
            "ðŸ’¡ Elite Enhanced Strategy Features:\n",
            "   â€¢ Advanced session sequence analysis with complexity scoring\n",
            "   â€¢ Enhanced credit behavior patterns and limit utilization\n",
            "   â€¢ Behavioral regime detection with persistence tracking\n",
            "   â€¢ Dynamic ensemble weighting based on customer profiles\n",
            "   â€¢ Comprehensive interaction feature engineering\n",
            "   â€¢ Multi-dimensional risk scoring integration\n",
            "   â€¢ Temporal cross-validation with business constraints\n",
            "   â€¢ Advanced feature selection (60 top features)\n",
            "   â€¢ Session-credit interaction modeling\n",
            "   â€¢ Real-time threshold optimization\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "    print(\"XGBoost not available, using alternative models\")\n",
        "\n",
        "print(\"ðŸš€ Loading and preparing data for elite performance...\")\n",
        "\n",
        "# Load data\n",
        "customers = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customers_all.csv\")\n",
        "accounts = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/accounts_all.csv\")\n",
        "transactions_train = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_train.csv\")\n",
        "transactions_test = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/transactions_test.csv\")\n",
        "train_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_train.csv\")\n",
        "test_panel = pd.read_csv(\"/home/jovyan/shared/datasets/RetailBanking/customer_panel_test.csv\")\n",
        "\n",
        "with open(\"/home/jovyan/shared/datasets/RetailBanking/device_sessions_all.json\", 'r') as f:\n",
        "    import json\n",
        "    device_sessions = pd.json_normalize(json.load(f))\n",
        "\n",
        "print(\"ðŸŽ¯ Creating elite feature engineering pipeline...\")\n",
        "\n",
        "def create_elite_customer_features(customers, accounts):\n",
        "    \"\"\"Create sophisticated customer segmentation and risk profiling\"\"\"\n",
        "    features = customers.copy()\n",
        "\n",
        "    # Advanced encoding\n",
        "    le_city = LabelEncoder()\n",
        "    features['HomeCity_encoded'] = le_city.fit_transform(features['HomeCity'].fillna('Unknown'))\n",
        "\n",
        "    # Account portfolio analysis\n",
        "    account_metrics = accounts.groupby('CustomerID').agg({\n",
        "        'AccountID': 'count',\n",
        "        'Balance': ['sum', 'mean', 'std', 'max', 'min', 'median'],\n",
        "        'Limit': ['sum', 'max', 'mean'],\n",
        "        'Type': lambda x: x.nunique()\n",
        "    }).reset_index()\n",
        "    account_metrics.columns = ['CustomerID', 'total_accounts', 'balance_sum', 'balance_mean',\n",
        "                              'balance_std', 'balance_max', 'balance_min', 'balance_median',\n",
        "                              'limit_sum', 'limit_max', 'limit_mean', 'account_type_diversity']\n",
        "\n",
        "    # Credit-specific features\n",
        "    credit_accounts = accounts[accounts['Type'] == 'credit_card']\n",
        "    if not credit_accounts.empty:\n",
        "        credit_metrics = credit_accounts.groupby('CustomerID').agg({\n",
        "            'Balance': ['sum', 'mean', 'max'],\n",
        "            'Limit': ['sum', 'max', 'mean']\n",
        "        }).reset_index()\n",
        "        credit_metrics.columns = ['CustomerID', 'credit_balance_sum', 'credit_balance_mean',\n",
        "                                 'credit_balance_max', 'credit_limit_sum', 'credit_limit_max',\n",
        "                                 'credit_limit_mean']\n",
        "\n",
        "        credit_metrics['credit_utilization'] = credit_metrics['credit_balance_sum'] / (credit_metrics['credit_limit_sum'] + 1)\n",
        "        credit_metrics['max_credit_utilization'] = credit_metrics['credit_balance_max'] / (credit_metrics['credit_limit_max'] + 1)\n",
        "    else:\n",
        "        credit_metrics = pd.DataFrame(columns=['CustomerID', 'credit_utilization', 'max_credit_utilization'])\n",
        "\n",
        "    # Account type composition\n",
        "    account_composition = pd.get_dummies(accounts[['CustomerID', 'Type']], columns=['Type'], prefix='account')\n",
        "    account_composition = account_composition.groupby('CustomerID').sum().reset_index()\n",
        "\n",
        "    # Merge features\n",
        "    features = features.merge(account_metrics, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in credit_metrics.columns:\n",
        "        features = features.merge(credit_metrics, on='CustomerID', how='left')\n",
        "    features = features.merge(account_composition, on='CustomerID', how='left')\n",
        "\n",
        "    # Advanced financial ratios\n",
        "    features['balance_to_salary'] = features['balance_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['limit_to_salary'] = features['limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['credit_depth'] = features['credit_limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "\n",
        "    # Risk profiling\n",
        "    features['high_risk_profile'] = (\n",
        "        (features['CreditScore'] < 580).astype(int) * 0.4 +\n",
        "        (features.get('credit_utilization', 0) > 0.8).astype(int) * 0.3 +\n",
        "        (features['balance_to_salary'] > 0.5).astype(int) * 0.3\n",
        "    )\n",
        "\n",
        "    # Customer lifetime value proxy\n",
        "    features['clv_score'] = (\n",
        "        (features['AnnualSalary'] / features['AnnualSalary'].max()) * 0.4 +\n",
        "        (features['CreditScore'] / 850) * 0.3 +\n",
        "        (features['Tenure'] / features['Tenure'].max()) * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def enhance_session_intelligence(device_sessions):\n",
        "    \"\"\"Build on your successful session features\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "\n",
        "    # Action sequence analysis - FIXED VERSION\n",
        "    def analyze_action_sequences(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'action_sequence_complexity': 0,\n",
        "                'financial_action_velocity': 0,\n",
        "                'sensitive_action_clustering': 0\n",
        "            }\n",
        "\n",
        "            financial_actions = []\n",
        "            action_types = []\n",
        "\n",
        "            for i, action in enumerate(actions_list):\n",
        "                # Handle both string and dictionary actions\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', 'unknown')\n",
        "                    action_types.append(action_type)\n",
        "\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        financial_actions.append(i)\n",
        "                elif isinstance(action, str):\n",
        "                    action_types.append(action)\n",
        "                    if action in ['transfer', 'payment']:\n",
        "                        financial_actions.append(i)\n",
        "\n",
        "            # Calculate financial action velocity\n",
        "            if len(financial_actions) > 1:\n",
        "                stats['financial_action_velocity'] = len(financial_actions) / (financial_actions[-1] - financial_actions[0] + 1)\n",
        "\n",
        "            # Action sequence complexity (entropy)\n",
        "            if action_types:\n",
        "                unique, counts = np.unique(action_types, return_counts=True)\n",
        "                probs = counts / len(action_types)\n",
        "                stats['action_sequence_complexity'] = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "            return stats\n",
        "        return {'action_sequence_complexity': 0, 'financial_action_velocity': 0, 'sensitive_action_clustering': 0}\n",
        "\n",
        "    action_sequence_analysis = sessions['Actions'].apply(analyze_action_sequences)\n",
        "    sequence_df = pd.DataFrame(action_sequence_analysis.tolist())\n",
        "    sequence_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    return sequence_df.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "def enhance_credit_behavior_features(panel_df, accounts):\n",
        "    \"\"\"Leverage your successful credit risk features\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Credit utilization patterns\n",
        "    credit_accounts = accounts[accounts['Type'] == 'credit_card']\n",
        "    if not credit_accounts.empty:\n",
        "        credit_behavior = credit_accounts.groupby('CustomerID').agg({\n",
        "            'Balance': ['mean', 'std', 'skew'],\n",
        "            'Limit': ['mean', 'min', 'max']\n",
        "        }).reset_index()\n",
        "        credit_behavior.columns = ['CustomerID', 'credit_balance_mean', 'credit_balance_std', 'credit_balance_skew',\n",
        "                                 'credit_limit_mean', 'credit_limit_min', 'credit_limit_max']\n",
        "\n",
        "        # Credit limit utilization patterns\n",
        "        credit_behavior['limit_utilization_variance'] = credit_behavior['credit_balance_std'] / (credit_behavior['credit_limit_mean'] + 1)\n",
        "        credit_behavior['limit_adequacy'] = credit_behavior['credit_limit_mean'] / (credit_behavior['credit_limit_max'] + 1)\n",
        "\n",
        "        features = features.merge(credit_behavior, on='CustomerID', how='left')\n",
        "\n",
        "    # Payment behavior sophistication\n",
        "    features['payment_consistency'] = 1 - features.groupby('CustomerID')['PaymentRatio'].transform('std').fillna(0)\n",
        "    features['utilization_stability'] = 1 - features.groupby('CustomerID')['Utilisation'].transform('std').fillna(0)\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def detect_behavioral_regimes(panel_df):\n",
        "    \"\"\"Detect different behavioral regimes in customer patterns\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Regime detection based on utilization patterns\n",
        "    features['high_utilization_regime'] = (\n",
        "        features['Utilisation'].rolling(3, min_periods=1).mean() > 0.7\n",
        "    ).astype(int)\n",
        "\n",
        "    features['deteriorating_regime'] = (\n",
        "        (features['Utilisation'].diff(2) > 0.1) &\n",
        "        (features['PaymentRatio'].diff(2) < -0.1)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Regime persistence\n",
        "    for customer_id in features['CustomerID'].unique():\n",
        "        cust_mask = features['CustomerID'] == customer_id\n",
        "        features.loc[cust_mask, 'regime_persistence'] = (\n",
        "            features.loc[cust_mask, 'high_utilization_regime'].rolling(4, min_periods=1).mean()\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "def enhance_temporal_features(panel_df):\n",
        "    \"\"\"Add more sophisticated time-series features\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Seasonal decomposition\n",
        "    features['utilization_seasonal'] = features.groupby('CustomerID')['Utilisation'].transform(\n",
        "        lambda x: x - x.rolling(4, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "    # Change point detection\n",
        "    features['utilization_change_point'] = (\n",
        "        features['Utilisation'].rolling(3).std() > features['Utilisation'].rolling(6).std() * 1.5\n",
        "    ).astype(int)\n",
        "\n",
        "    # Momentum indicators\n",
        "    features['utilization_momentum'] = features['Utilisation'] - features['Utilisation'].shift(2)\n",
        "    features['payment_momentum'] = features['PaymentRatio'] - features['PaymentRatio'].shift(2)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_temporal_dynamics_features(panel_df):\n",
        "    \"\"\"Create sophisticated time-series features with trend analysis\"\"\"\n",
        "    temporal_features = []\n",
        "\n",
        "    for customer_id in panel_df['CustomerID'].unique():\n",
        "        cust_data = panel_df[panel_df['CustomerID'] == customer_id].sort_values('Week')\n",
        "\n",
        "        # Rolling statistics with multiple windows\n",
        "        for window in [2, 3, 4]:\n",
        "            cust_data[f'utilization_ma_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'payment_ratio_ma_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'utilization_std_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).std()\n",
        "            cust_data[f'payment_ratio_std_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).std()\n",
        "\n",
        "        # Trend analysis\n",
        "        cust_data['utilization_trend_3'] = cust_data['Utilisation'].diff(periods=2).fillna(0)\n",
        "        cust_data['payment_trend_3'] = cust_data['PaymentRatio'].diff(periods=2).fillna(0)\n",
        "\n",
        "        # Acceleration (second derivative)\n",
        "        cust_data['utilization_acceleration'] = cust_data['utilization_trend_3'].diff().fillna(0)\n",
        "        cust_data['payment_acceleration'] = cust_data['payment_trend_3'].diff().fillna(0)\n",
        "\n",
        "        # Volatility measures\n",
        "        cust_data['utilization_volatility'] = cust_data['Utilisation'].rolling(4, min_periods=1).std()\n",
        "        cust_data['payment_volatility'] = cust_data['PaymentRatio'].rolling(4, min_periods=1).std()\n",
        "\n",
        "        # Behavioral patterns\n",
        "        cust_data['high_utilization_streak'] = (cust_data['Utilisation'] > 0.7).astype(int)\n",
        "        cust_data['low_payment_streak'] = (cust_data['PaymentRatio'] < 0.2).astype(int)\n",
        "\n",
        "        # Calculate streaks\n",
        "        for i in range(1, len(cust_data)):\n",
        "            if cust_data.iloc[i]['high_utilization_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('high_utilization_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['high_utilization_streak'] + 1\n",
        "            if cust_data.iloc[i]['low_payment_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('low_payment_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['low_payment_streak'] + 1\n",
        "\n",
        "        # Deterioration indicators\n",
        "        cust_data['financial_deterioration'] = (\n",
        "            (cust_data['utilization_trend_3'] > 0).astype(int) * 0.5 +\n",
        "            (cust_data['payment_trend_3'] < 0).astype(int) * 0.5\n",
        "        )\n",
        "\n",
        "        temporal_features.append(cust_data)\n",
        "\n",
        "    result = pd.concat(temporal_features, ignore_index=True).fillna(0)\n",
        "\n",
        "    # Apply enhanced temporal features\n",
        "    result = enhance_temporal_features(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_advanced_transaction_features(transactions_df):\n",
        "    \"\"\"Create comprehensive transaction behavior profiling\"\"\"\n",
        "    transactions = transactions_df.copy()\n",
        "    transactions['Timestamp'] = pd.to_datetime(transactions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Basic transaction metrics\n",
        "    txn_metrics = transactions.groupby('CustomerID').agg({\n",
        "        'TxnID': 'count',\n",
        "        'Amount': ['sum', 'mean', 'std', 'max', 'min', 'median', 'skew'],\n",
        "        'Timestamp': ['min', 'max', 'nunique']\n",
        "    }).reset_index()\n",
        "    txn_metrics.columns = ['CustomerID', 'txn_count', 'amount_sum', 'amount_mean', 'amount_std',\n",
        "                          'amount_max', 'amount_min', 'amount_median', 'amount_skew',\n",
        "                          'first_txn', 'last_txn', 'active_days']\n",
        "\n",
        "    # Transaction period and velocity\n",
        "    txn_metrics['txn_period_days'] = (txn_metrics['last_txn'] - txn_metrics['first_txn']).dt.days + 1\n",
        "    txn_metrics['daily_txn_frequency'] = txn_metrics['txn_count'] / txn_metrics['txn_period_days']\n",
        "    txn_metrics['daily_spending'] = txn_metrics['amount_sum'] / txn_metrics['txn_period_days']\n",
        "\n",
        "    # Large transaction analysis\n",
        "    large_txn_threshold = transactions['Amount'].quantile(0.8)\n",
        "    large_txns = transactions[transactions['Amount'] > large_txn_threshold]\n",
        "\n",
        "    if not large_txns.empty:\n",
        "        large_txn_stats = large_txns.groupby('CustomerID').agg({\n",
        "            'TxnID': 'count',\n",
        "            'Amount': ['mean', 'sum', 'max']\n",
        "        }).reset_index()\n",
        "        large_txn_stats.columns = ['CustomerID', 'large_txn_count', 'large_txn_avg',\n",
        "                                  'large_txn_sum', 'large_txn_max']\n",
        "\n",
        "        large_txn_stats['large_txn_ratio'] = large_txn_stats['large_txn_count'] / txn_metrics['txn_count']\n",
        "        large_txn_stats['large_amount_ratio'] = large_txn_stats['large_txn_sum'] / txn_metrics['amount_sum']\n",
        "    else:\n",
        "        large_txn_stats = pd.DataFrame(columns=['CustomerID', 'large_txn_ratio', 'large_amount_ratio'])\n",
        "\n",
        "    # Channel behavior\n",
        "    channel_behavior = pd.get_dummies(transactions[['CustomerID', 'Channel']],\n",
        "                                    columns=['Channel'], prefix='channel')\n",
        "    channel_behavior = channel_behavior.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "    # MCC spending patterns\n",
        "    if 'MCC_Group' in transactions.columns:\n",
        "        mcc_behavior = pd.get_dummies(transactions[['CustomerID', 'MCC_Group']],\n",
        "                                    columns=['MCC_Group'], prefix='mcc')\n",
        "        mcc_behavior = mcc_behavior.groupby('CustomerID').mean().reset_index()\n",
        "    else:\n",
        "        mcc_behavior = pd.DataFrame(columns=['CustomerID'])\n",
        "\n",
        "    # Temporal patterns\n",
        "    transactions['hour'] = transactions['Timestamp'].dt.hour\n",
        "    transactions['day_of_week'] = transactions['Timestamp'].dt.dayofweek\n",
        "    transactions['is_weekend'] = (transactions['day_of_week'] >= 5).astype(int)\n",
        "    transactions['is_night'] = ((transactions['hour'] >= 22) | (transactions['hour'] <= 6)).astype(int)\n",
        "\n",
        "    temporal_patterns = transactions.groupby('CustomerID').agg({\n",
        "        'is_weekend': 'mean',\n",
        "        'is_night': 'mean',\n",
        "        'hour': ['mean', 'std', lambda x: x.mode()[0] if len(x.mode()) > 0 else 12]\n",
        "    }).reset_index()\n",
        "    temporal_patterns.columns = ['CustomerID', 'weekend_ratio', 'night_ratio',\n",
        "                                'avg_txn_hour', 'std_txn_hour', 'mode_txn_hour']\n",
        "\n",
        "    # Spending consistency\n",
        "    daily_spending = transactions.groupby([transactions['Timestamp'].dt.date, 'CustomerID'])['Amount'].sum().reset_index()\n",
        "    spending_consistency = daily_spending.groupby('CustomerID')['Amount'].agg(['mean', 'std']).reset_index()\n",
        "    spending_consistency.columns = ['CustomerID', 'daily_spending_mean', 'daily_spending_std']\n",
        "    spending_consistency['spending_volatility'] = spending_consistency['daily_spending_std'] / (spending_consistency['daily_spending_mean'] + 1)\n",
        "\n",
        "    # Merge all features\n",
        "    features = txn_metrics.merge(channel_behavior, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in mcc_behavior.columns:\n",
        "        features = features.merge(mcc_behavior, on='CustomerID', how='left')\n",
        "    features = features.merge(temporal_patterns, on='CustomerID', how='left')\n",
        "    features = features.merge(spending_consistency, on='CustomerID', how='left')\n",
        "\n",
        "    if 'CustomerID' in large_txn_stats.columns:\n",
        "        features = features.merge(large_txn_stats, on='CustomerID', how='left')\n",
        "\n",
        "    # Risk scores\n",
        "    features['transaction_risk_score'] = (\n",
        "        features.get('large_txn_ratio', 0) * 0.3 +\n",
        "        features['spending_volatility'] * 0.3 +\n",
        "        features['night_ratio'] * 0.2 +\n",
        "        (features['amount_skew'].abs() * 0.2)\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_session_intelligence_features(device_sessions):\n",
        "    \"\"\"Create advanced session behavior intelligence\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "    sessions['Timestamp'] = pd.to_datetime(sessions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Session frequency and patterns\n",
        "    session_freq = sessions.groupby('CustomerID').agg({\n",
        "        'SessionID': 'count',\n",
        "        'City': 'nunique',\n",
        "        'DeviceID': 'nunique',\n",
        "        'IP': 'nunique',\n",
        "        'Timestamp': ['min', 'max']\n",
        "    }).reset_index()\n",
        "    session_freq.columns = ['CustomerID', 'session_count', 'cities_visited', 'devices_used',\n",
        "                           'ips_used', 'first_session', 'last_session']\n",
        "\n",
        "    session_freq['session_period_days'] = (session_freq['last_session'] - session_freq['first_session']).dt.days + 1\n",
        "    session_freq['daily_sessions'] = session_freq['session_count'] / session_freq['session_period_days']\n",
        "\n",
        "    # Advanced action analysis - FIXED VERSION\n",
        "    def analyze_advanced_actions(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'total_actions': len(actions_list),\n",
        "                'financial_actions': 0,\n",
        "                'sensitive_actions': 0,\n",
        "                'login_count': 0,\n",
        "                'transfer_amount': 0,\n",
        "                'payment_amount': 0,\n",
        "                'unique_action_types': set()\n",
        "            }\n",
        "\n",
        "            for action in actions_list:\n",
        "                # Handle both string and dictionary actions\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', '')\n",
        "                    stats['unique_action_types'].add(action_type)\n",
        "\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                        amount = action.get('amount', 0)\n",
        "                        if action_type == 'transfer':\n",
        "                            stats['transfer_amount'] += amount\n",
        "                        else:\n",
        "                            stats['payment_amount'] += amount\n",
        "                    elif action_type == 'account_view':\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                elif isinstance(action, str):\n",
        "                    stats['unique_action_types'].add(action)\n",
        "                    if action == 'login':\n",
        "                        stats['login_count'] += 1\n",
        "                    elif action in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                    elif action == 'account_view':\n",
        "                        stats['sensitive_actions'] += 1\n",
        "\n",
        "            stats['unique_action_count'] = len(stats['unique_action_types'])\n",
        "            return stats\n",
        "        return {'total_actions': 0, 'financial_actions': 0, 'sensitive_actions': 0,\n",
        "                'login_count': 0, 'transfer_amount': 0, 'payment_amount': 0, 'unique_action_count': 0}\n",
        "\n",
        "    action_analysis = sessions['Actions'].apply(analyze_advanced_actions)\n",
        "    action_df = pd.DataFrame(action_analysis.tolist())\n",
        "    action_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    # Aggregate action intelligence\n",
        "    action_intel = action_df.groupby('CustomerID').agg({\n",
        "        'total_actions': 'sum',\n",
        "        'financial_actions': 'sum',\n",
        "        'sensitive_actions': 'sum',\n",
        "        'login_count': 'sum',\n",
        "        'transfer_amount': 'sum',\n",
        "        'payment_amount': 'sum',\n",
        "        'unique_action_count': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate behavioral ratios\n",
        "    action_intel['financial_action_ratio'] = action_intel['financial_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['sensitive_action_ratio'] = action_intel['sensitive_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['login_frequency'] = action_intel['login_count'] / session_freq['session_count']\n",
        "    action_intel['avg_financial_amount'] = (action_intel['transfer_amount'] + action_intel['payment_amount']) / (action_intel['financial_actions'] + 1)\n",
        "\n",
        "    # Merge features\n",
        "    features = session_freq.merge(action_intel, on='CustomerID', how='left')\n",
        "\n",
        "    # Security and risk indicators\n",
        "    features['geographic_dispersion'] = features['cities_visited'] / (features['session_count'] + 1)\n",
        "    features['device_diversity'] = features['devices_used'] / (features['session_count'] + 1)\n",
        "    features['ip_diversity'] = features['ips_used'] / (features['session_count'] + 1)\n",
        "\n",
        "    features['session_risk_score'] = (\n",
        "        features['geographic_dispersion'] * 0.3 +\n",
        "        features['device_diversity'] * 0.3 +\n",
        "        features['sensitive_action_ratio'] * 0.4\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_interaction_features(features):\n",
        "    \"\"\"Create powerful interaction terms\"\"\"\n",
        "    # Financial stress interactions\n",
        "    if all(col in features.columns for col in ['utilization_ma_3', 'payment_ratio_ma_3']):\n",
        "        features['utilization_payment_interaction'] = (\n",
        "            features['utilization_ma_3'] * (1 - features['payment_ratio_ma_3'])\n",
        "        )\n",
        "\n",
        "    # Credit-behavior interactions\n",
        "    if all(col in features.columns for col in ['CreditScore', 'transaction_risk_score']):\n",
        "        features['credit_behavior_risk'] = (\n",
        "            (1 - features['CreditScore'] / 850) * features['transaction_risk_score']\n",
        "        )\n",
        "\n",
        "    # Multi-dimensional risk scoring\n",
        "    risk_components = []\n",
        "    risk_cols = ['high_risk_profile', 'transaction_risk_score', 'session_risk_score', 'financial_deterioration']\n",
        "\n",
        "    for col in risk_cols:\n",
        "        if col in features.columns:\n",
        "            risk_components.append(features[col])\n",
        "\n",
        "    if risk_components:\n",
        "        features['comprehensive_risk_index'] = sum(risk_components) / len(risk_components)\n",
        "\n",
        "    # Session-credit interactions\n",
        "    if all(col in features.columns for col in ['unique_action_count', 'CreditScore']):\n",
        "        features['session_credit_interaction'] = (\n",
        "            features['unique_action_count'] * (1 - features['CreditScore'] / 850)\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"ðŸ”„ Building elite feature sets...\")\n",
        "\n",
        "# Create all feature sets\n",
        "customer_features = create_elite_customer_features(customers, accounts)\n",
        "temporal_train = create_temporal_dynamics_features(train_panel)\n",
        "temporal_test = create_temporal_dynamics_features(test_panel)\n",
        "\n",
        "# Combine train and test transactions for consistent feature engineering\n",
        "all_transactions = pd.concat([transactions_train, transactions_test])\n",
        "transaction_features = create_advanced_transaction_features(all_transactions)\n",
        "session_features = create_session_intelligence_features(device_sessions)\n",
        "\n",
        "# Enhanced features\n",
        "enhanced_session_features = enhance_session_intelligence(device_sessions)\n",
        "\n",
        "print(\"ðŸŽ¯ Creating final feature matrix...\")\n",
        "\n",
        "def create_elite_feature_matrix(panel_df, customer_df, temporal_df, transaction_df, session_df, enhanced_session_df):\n",
        "    \"\"\"Combine all elite features with intelligent interactions\"\"\"\n",
        "    features = temporal_df.copy()\n",
        "\n",
        "    # Merge all feature sources\n",
        "    features = features.merge(customer_df, on='CustomerID', how='left')\n",
        "    features = features.merge(transaction_df, on='CustomerID', how='left')\n",
        "    features = features.merge(session_df, on='CustomerID', how='left')\n",
        "    features = features.merge(enhanced_session_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Enhanced credit behavior features\n",
        "    features = enhance_credit_behavior_features(features, accounts)\n",
        "\n",
        "    # Behavioral regime detection\n",
        "    features = detect_behavioral_regimes(features)\n",
        "\n",
        "    # Create powerful interaction features\n",
        "    # Financial stress indicators\n",
        "    stress_components = []\n",
        "    if 'utilization_ma_3' in features.columns:\n",
        "        stress_components.append(features['utilization_ma_3'] * 0.25)\n",
        "    if 'payment_ratio_ma_3' in features.columns:\n",
        "        stress_components.append((1 - features['payment_ratio_ma_3']) * 0.25)\n",
        "    if 'high_risk_profile' in features.columns:\n",
        "        stress_components.append(features['high_risk_profile'] * 0.25)\n",
        "    if 'transaction_risk_score' in features.columns:\n",
        "        stress_components.append(features['transaction_risk_score'] * 0.25)\n",
        "\n",
        "    if stress_components:\n",
        "        features['comprehensive_stress_score'] = sum(stress_components)\n",
        "\n",
        "    # Behavioral risk indicators\n",
        "    behavior_components = []\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        behavior_components.append(features['session_risk_score'] * 0.4)\n",
        "    if 'financial_deterioration' in features.columns:\n",
        "        behavior_components.append(features['financial_deterioration'] * 0.3)\n",
        "    if 'spending_volatility' in features.columns:\n",
        "        behavior_components.append(features['spending_volatility'] * 0.3)\n",
        "\n",
        "    if behavior_components:\n",
        "        features['behavioral_risk_score'] = sum(behavior_components)\n",
        "\n",
        "    # Credit capacity indicators\n",
        "    if 'CreditScore' in features.columns and 'credit_utilization' in features.columns:\n",
        "        features['credit_health_index'] = (\n",
        "            (features['CreditScore'] / 850) * 0.6 +\n",
        "            (1 - features['credit_utilization'].clip(0, 1)) * 0.4\n",
        "        )\n",
        "\n",
        "    # Payment behavior indicators\n",
        "    if 'payment_trend_3' in features.columns and 'payment_volatility' in features.columns:\n",
        "        features['payment_behavior_score'] = (\n",
        "            (1 - features['payment_trend_3'].clip(-1, 0).abs()) * 0.5 +\n",
        "            (1 - features['payment_volatility'].clip(0, 1)) * 0.5\n",
        "        )\n",
        "\n",
        "    # Apply interaction features\n",
        "    features = create_interaction_features(features)\n",
        "\n",
        "    # Final composite risk score\n",
        "    risk_components_final = []\n",
        "    if 'comprehensive_stress_score' in features.columns:\n",
        "        risk_components_final.append(features['comprehensive_stress_score'] * 0.3)\n",
        "    if 'behavioral_risk_score' in features.columns:\n",
        "        risk_components_final.append(features['behavioral_risk_score'] * 0.3)\n",
        "    if 'comprehensive_risk_index' in features.columns:\n",
        "        risk_components_final.append(features['comprehensive_risk_index'] * 0.2)\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        risk_components_final.append(features['session_risk_score'] * 0.2)\n",
        "\n",
        "    if risk_components_final:\n",
        "        features['final_risk_score'] = sum(risk_components_final)\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "# Create final datasets\n",
        "X_train_elite = create_elite_feature_matrix(train_panel, customer_features, temporal_train, transaction_features, session_features, enhanced_session_features)\n",
        "X_test_elite = create_elite_feature_matrix(test_panel, customer_features, temporal_test, transaction_features, session_features, enhanced_session_features)\n",
        "\n",
        "# Prepare for modeling\n",
        "y_train = X_train_elite['DefaultLabel'].astype(int)\n",
        "non_feature_cols = ['CustomerID', 'Week', 'DefaultLabel', 'first_txn', 'last_txn', 'first_session', 'last_session']\n",
        "feature_cols = [col for col in X_train_elite.columns if col not in non_feature_cols]\n",
        "\n",
        "X_train = X_train_elite[feature_cols]\n",
        "X_test = X_test_elite[feature_cols]\n",
        "\n",
        "print(f\"âœ… Elite feature matrix: {X_train.shape[1]} features, {X_train.shape[0]} samples\")\n",
        "\n",
        "# Ensure numeric types and handle infinite values\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(\"ðŸ” Performing elite feature selection...\")\n",
        "\n",
        "# Use Random Forest for feature selection\n",
        "selector_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "selector_model.fit(X_train, y_train)\n",
        "\n",
        "# Select top features based on importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': selector_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Keep top 60 features for enhanced model\n",
        "top_features = feature_importance.head(60)['feature'].tolist()\n",
        "X_train_selected = X_train[top_features]\n",
        "X_test_selected = X_test[top_features]\n",
        "\n",
        "print(f\"ðŸŽ¯ Selected {len(top_features)} most predictive features\")\n",
        "\n",
        "def optimize_threshold_with_business_rules(y_true, y_proba, min_default_rate=0.05, max_default_rate=0.15):\n",
        "    \"\"\"Optimize threshold considering business constraints\"\"\"\n",
        "    thresholds = np.linspace(0.1, 0.5, 100)\n",
        "    best_threshold = 0.25\n",
        "    best_f1 = 0\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_proba > threshold).astype(int)\n",
        "        default_rate = y_pred.mean()\n",
        "\n",
        "        # Apply business constraints\n",
        "        if min_default_rate <= default_rate <= max_default_rate:\n",
        "            f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_threshold, best_f1\n",
        "\n",
        "def dynamic_ensemble_weighting(rf_proba, gb_proba, customer_features):\n",
        "    \"\"\"Dynamic ensemble weighting based on customer characteristics\"\"\"\n",
        "    # Default weights\n",
        "    default_rf_weight = 0.7\n",
        "\n",
        "    # If we have session complexity features, adjust weights\n",
        "    if 'action_sequence_complexity' in customer_features.columns:\n",
        "        session_complexity = customer_features['action_sequence_complexity'].fillna(0)\n",
        "        # Higher RF weight for complex session behavior\n",
        "        rf_weights = np.where(\n",
        "            session_complexity > session_complexity.quantile(0.7),\n",
        "            0.8,  # More RF for complex sessions\n",
        "            default_rf_weight\n",
        "        )\n",
        "    else:\n",
        "        rf_weights = np.full(len(rf_proba), default_rf_weight)\n",
        "\n",
        "    # If we have credit scores, adjust weights\n",
        "    if 'CreditScore' in customer_features.columns:\n",
        "        credit_scores = customer_features['CreditScore'].fillna(650)\n",
        "        # Higher GB weight for strong credit profiles\n",
        "        credit_adjustment = np.where(\n",
        "            credit_scores > 700,\n",
        "            -0.1,  # Less RF for good credit\n",
        "            0.1    # More RF for poor credit\n",
        "        )\n",
        "        rf_weights = np.clip(rf_weights + credit_adjustment, 0.5, 0.9)\n",
        "\n",
        "    # Apply dynamic weighting\n",
        "    ensemble_proba = np.zeros_like(rf_proba)\n",
        "    for i in range(len(ensemble_proba)):\n",
        "        ensemble_proba[i] = (rf_proba[i] * rf_weights[i] +\n",
        "                           gb_proba[i] * (1 - rf_weights[i]))\n",
        "\n",
        "    return ensemble_proba\n",
        "\n",
        "def create_stacked_ensemble(X_train, y_train, X_test):\n",
        "    \"\"\"Create stacked ensemble with multiple base models\"\"\"\n",
        "    base_models = {\n",
        "        'rf': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "        'gb': GradientBoostingClassifier(n_estimators=150, random_state=42)\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGB_AVAILABLE:\n",
        "        base_models['xgb'] = xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Create meta-features using cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    meta_train = np.zeros((X_train.shape[0], len(base_models)))\n",
        "    meta_test = np.zeros((X_test.shape[0], len(base_models)))\n",
        "\n",
        "    for i, (name, model) in enumerate(base_models.items()):\n",
        "        fold_predictions = np.zeros(X_train.shape[0])\n",
        "        test_predictions = []\n",
        "\n",
        "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "            model.fit(X_train[train_idx], y_train[train_idx])\n",
        "            fold_predictions[val_idx] = model.predict_proba(X_train[val_idx])[:, 1]\n",
        "            test_predictions.append(model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "        meta_train[:, i] = fold_predictions\n",
        "        meta_test[:, i] = np.mean(test_predictions, axis=0)\n",
        "\n",
        "    # Meta-model\n",
        "    meta_model = LogisticRegression()\n",
        "    meta_model.fit(meta_train, y_train)\n",
        "\n",
        "    return meta_model.predict_proba(meta_test)[:, 1]\n",
        "\n",
        "def temporal_cross_validation(model, X, y, weeks, n_splits=5):\n",
        "    \"\"\"Time-aware cross-validation\"\"\"\n",
        "    unique_weeks = sorted(weeks.unique())\n",
        "    split_size = len(unique_weeks) // n_splits\n",
        "\n",
        "    scores = []\n",
        "    thresholds = []\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        val_weeks = unique_weeks[i * split_size:(i + 1) * split_size]\n",
        "\n",
        "        train_mask = ~weeks.isin(val_weeks)\n",
        "        val_mask = weeks.isin(val_weeks)\n",
        "\n",
        "        X_train_fold, X_val = X[train_mask], X[val_mask]\n",
        "        y_train_fold, y_val = y[train_mask], y[val_mask]\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        y_pred = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # Optimize threshold for this fold\n",
        "        threshold, _ = optimize_threshold_with_business_rules(y_val, y_pred)\n",
        "        val_pred = (y_pred > threshold).astype(int)\n",
        "\n",
        "        score = f1_score(y_val, val_pred, average='macro')\n",
        "        scores.append(score)\n",
        "        thresholds.append(threshold)\n",
        "        print(f\"  Fold {i+1}: F1 = {score:.4f}, Threshold = {threshold:.3f}\")\n",
        "\n",
        "    return np.mean(scores), np.std(scores), np.mean(thresholds)\n",
        "\n",
        "print(\"ðŸ¤– Training elite ensemble model...\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Perform temporal cross-validation\n",
        "print(\"ðŸ“Š Performing temporal cross-validation...\")\n",
        "rf_cv_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "cv_score, cv_std, avg_threshold = temporal_cross_validation(rf_cv_model, X_train_scaled, y_train, X_train_elite['Week'])\n",
        "print(f\"âœ… Cross-validation score: {cv_score:.4f} Â± {cv_std:.4f}\")\n",
        "print(f\"âœ… Average optimal threshold: {avg_threshold:.3f}\")\n",
        "\n",
        "# Train optimized ensemble\n",
        "print(\"ðŸŽ¯ Training final ensemble models...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=20,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced_subsample',\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train both models\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get base predictions\n",
        "rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "gb_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Use dynamic ensemble weighting\n",
        "ensemble_proba = dynamic_ensemble_weighting(rf_proba, gb_proba, X_test_elite)\n",
        "\n",
        "print(\"ðŸ“Š Analyzing optimal threshold...\")\n",
        "\n",
        "# Find optimal threshold for Macro-F1 with business constraints\n",
        "optimal_threshold, optimal_f1 = optimize_threshold_with_business_rules(\n",
        "    y_train,\n",
        "    rf_model.predict_proba(X_train_scaled)[:, 1]  # Use RF for threshold optimization\n",
        ")\n",
        "\n",
        "print(f\"âœ… Optimal threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"   Expected F1 score: {optimal_f1:.4f}\")\n",
        "\n",
        "# Final predictions\n",
        "final_predictions = (ensemble_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'CustomerID': test_panel['CustomerID'],\n",
        "    'Week': test_panel['Week'],\n",
        "    'DefaultLabel': final_predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"retailbanking_challenge2_elite_enhanced_predictions.csv\", index=False)\n",
        "\n",
        "print(f\"ðŸŽ‰ Elite enhanced predictions saved: {len(final_predictions)} predictions\")\n",
        "print(f\"   Default rate: {final_predictions.mean():.3f} ({final_predictions.sum()} defaults)\")\n",
        "\n",
        "# Model interpretation\n",
        "def explain_predictions(model, feature_names, top_n=20):\n",
        "    \"\"\"Provide business-interpretable feature importance\"\"\"\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(top_n)\n",
        "\n",
        "    # Categorize features for business interpretation\n",
        "    categories = {\n",
        "        'Credit Risk': ['CreditScore', 'credit_utilization', 'high_risk_profile', 'credit_health'],\n",
        "        'Payment Behavior': ['PaymentRatio', 'payment_trend', 'payment_volatility', 'payment_consistency'],\n",
        "        'Spending Patterns': ['utilization_ma', 'transaction_risk_score', 'spending_volatility'],\n",
        "        'Session Behavior': ['session_risk_score', 'financial_actions', 'sensitive_action_ratio', 'action_sequence'],\n",
        "        'Behavioral Regimes': ['regime', 'deteriorating', 'persistence'],\n",
        "        'Composite Scores': ['comprehensive_risk', 'final_risk', 'stress_score']\n",
        "    }\n",
        "\n",
        "    for feature in importance_df['feature']:\n",
        "        category_found = False\n",
        "        for category, keywords in categories.items():\n",
        "            if any(keyword in feature for keyword in keywords):\n",
        "                importance_df.loc[importance_df['feature'] == feature, 'category'] = category\n",
        "                category_found = True\n",
        "                break\n",
        "        if not category_found:\n",
        "            importance_df.loc[importance_df['feature'] == feature, 'category'] = 'Other'\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "print(\"ðŸ” Top predictive features:\")\n",
        "feature_explanation = explain_predictions(rf_model, top_features)\n",
        "print(feature_explanation[['feature', 'category', 'importance']].head(15))\n",
        "\n",
        "# Submit predictions\n",
        "try:\n",
        "    from agentds import BenchmarkClient\n",
        "    client = BenchmarkClient(api_key=\"adsb_E8N9aNAz2w1K7dNYT8BSMGsd_1760199769\", team_name=\"synergy-minds\")\n",
        "\n",
        "    result = client.submit_prediction(\"Retailbanking\", 2, \"retailbanking_challenge2_elite_enhanced_predictions.csv\")\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"ðŸ† Submission successful!\")\n",
        "        print(f\"   ðŸ“Š Score: {result['score']:.4f}\")\n",
        "        current_score = result['score']\n",
        "        if current_score < 0.95:\n",
        "            improvement_needed = 0.95 - current_score\n",
        "            print(f\"   ðŸŽ¯ Need improvement: {improvement_needed:.4f} to reach elite tier\")\n",
        "        else:\n",
        "            print(\"   ðŸ’ª Excellent! You're in the elite tier!\")\n",
        "\n",
        "        # Compare with previous score\n",
        "        if current_score > 0.9438:\n",
        "            improvement = current_score - 0.9438\n",
        "            print(f\"   ðŸ“ˆ Improvement from previous: +{improvement:.4f}\")\n",
        "    else:\n",
        "        print(\"âŒ Submission failed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ’¥ Submission error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Elite Enhanced Strategy Features:\")\n",
        "print(\"   â€¢ Advanced session sequence analysis with complexity scoring\")\n",
        "print(\"   â€¢ Enhanced credit behavior patterns and limit utilization\")\n",
        "print(\"   â€¢ Behavioral regime detection with persistence tracking\")\n",
        "print(\"   â€¢ Dynamic ensemble weighting based on customer profiles\")\n",
        "print(\"   â€¢ Comprehensive interaction feature engineering\")\n",
        "print(\"   â€¢ Multi-dimensional risk scoring integration\")\n",
        "print(\"   â€¢ Temporal cross-validation with business constraints\")\n",
        "print(\"   â€¢ Advanced feature selection (60 top features)\")\n",
        "print(\"   â€¢ Session-credit interaction modeling\")\n",
        "print(\"   â€¢ Real-time threshold optimization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02xS7BsHNS7E"
      },
      "outputs": [],
      "source": [
        "#  Version for 0.9553\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set all random seeds for maximum reproducibility\n",
        "def set_all_seeds(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "set_all_seeds(42)\n",
        "\n",
        "print(\"ðŸš€ Loading and preparing data with enhanced stability...\")\n",
        "\n",
        "# Load data\n",
        "customers = pd.read_csv(\"customers_all.csv\")\n",
        "accounts = pd.read_csv(\"accounts_all.csv\")\n",
        "transactions_train = pd.read_csv(\"transactions_train.csv\")\n",
        "transactions_test = pd.read_csv(\"transactions_test.csv\")\n",
        "train_panel = pd.read_csv(\"customer_panel_train.csv\")\n",
        "test_panel = pd.read_csv(\"customer_panel_test.csv\")\n",
        "\n",
        "with open(\"device_sessions_all.json\", 'r') as f:\n",
        "    import json\n",
        "    device_sessions = pd.json_normalize(json.load(f))\n",
        "\n",
        "# STABLE FEATURE ENGINEERING FUNCTIONS (keep your existing functions but add stability)\n",
        "\n",
        "def create_elite_customer_features(customers, accounts):\n",
        "    \"\"\"Stable customer feature engineering\"\"\"\n",
        "    features = customers.copy()\n",
        "\n",
        "    # Stable encoding\n",
        "    le_city = LabelEncoder()\n",
        "    features['HomeCity_encoded'] = le_city.fit_transform(features['HomeCity'].fillna('Unknown'))\n",
        "\n",
        "    # Account portfolio analysis (same as before)\n",
        "    account_metrics = accounts.groupby('CustomerID').agg({\n",
        "        'AccountID': 'count',\n",
        "        'Balance': ['sum', 'mean', 'std', 'max', 'min', 'median'],\n",
        "        'Limit': ['sum', 'max', 'mean'],\n",
        "        'Type': lambda x: x.nunique()\n",
        "    }).reset_index()\n",
        "    account_metrics.columns = ['CustomerID', 'total_accounts', 'balance_sum', 'balance_mean',\n",
        "                              'balance_std', 'balance_max', 'balance_min', 'balance_median',\n",
        "                              'limit_sum', 'limit_max', 'limit_mean', 'account_type_diversity']\n",
        "\n",
        "    # Credit-specific features\n",
        "    credit_accounts = accounts[accounts['Type'] == 'credit_card']\n",
        "    if not credit_accounts.empty:\n",
        "        credit_metrics = credit_accounts.groupby('CustomerID').agg({\n",
        "            'Balance': ['sum', 'mean', 'max'],\n",
        "            'Limit': ['sum', 'max', 'mean']\n",
        "        }).reset_index()\n",
        "        credit_metrics.columns = ['CustomerID', 'credit_balance_sum', 'credit_balance_mean',\n",
        "                                 'credit_balance_max', 'credit_limit_sum', 'credit_limit_max',\n",
        "                                 'credit_limit_mean']\n",
        "        credit_metrics['credit_utilization'] = credit_metrics['credit_balance_sum'] / (credit_metrics['credit_limit_sum'] + 1)\n",
        "        credit_metrics['max_credit_utilization'] = credit_metrics['credit_balance_max'] / (credit_metrics['credit_limit_max'] + 1)\n",
        "    else:\n",
        "        credit_metrics = pd.DataFrame(columns=['CustomerID', 'credit_utilization', 'max_credit_utilization'])\n",
        "\n",
        "    # Account type composition\n",
        "    account_composition = pd.get_dummies(accounts[['CustomerID', 'Type']], columns=['Type'], prefix='account')\n",
        "    account_composition = account_composition.groupby('CustomerID').sum().reset_index()\n",
        "\n",
        "    # Merge features\n",
        "    features = features.merge(account_metrics, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in credit_metrics.columns:\n",
        "        features = features.merge(credit_metrics, on='CustomerID', how='left')\n",
        "    features = features.merge(account_composition, on='CustomerID', how='left')\n",
        "\n",
        "    # Advanced financial ratios\n",
        "    features['balance_to_salary'] = features['balance_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['limit_to_salary'] = features['limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "    features['credit_depth'] = features['credit_limit_sum'] / (features['AnnualSalary'] + 1)\n",
        "\n",
        "    # Risk profiling\n",
        "    features['high_risk_profile'] = (\n",
        "        (features['CreditScore'] < 580).astype(int) * 0.4 +\n",
        "        (features.get('credit_utilization', 0) > 0.8).astype(int) * 0.3 +\n",
        "        (features['balance_to_salary'] > 0.5).astype(int) * 0.3\n",
        "    )\n",
        "\n",
        "    # Customer lifetime value proxy\n",
        "    features['clv_score'] = (\n",
        "        (features['AnnualSalary'] / features['AnnualSalary'].max()) * 0.4 +\n",
        "        (features['CreditScore'] / 850) * 0.3 +\n",
        "        (features['Tenure'] / features['Tenure'].max()) * 0.3\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "# KEEP YOUR OTHER FEATURE ENGINEERING FUNCTIONS BUT ADD STABILITY\n",
        "# (enhance_session_intelligence, enhance_credit_behavior_features, detect_behavioral_regimes,\n",
        "# enhance_temporal_features, create_temporal_dynamics_features, create_advanced_transaction_features,\n",
        "# create_session_intelligence_features, create_interaction_features, create_elite_feature_matrix)\n",
        "\n",
        "\n",
        "def enhance_session_intelligence(device_sessions):\n",
        "    \"\"\"Build on your successful session features\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "\n",
        "    # Action sequence analysis - FIXED VERSION\n",
        "    def analyze_action_sequences(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'action_sequence_complexity': 0,\n",
        "                'financial_action_velocity': 0,\n",
        "                'sensitive_action_clustering': 0\n",
        "            }\n",
        "\n",
        "            financial_actions = []\n",
        "            action_types = []\n",
        "\n",
        "            for i, action in enumerate(actions_list):\n",
        "                # Handle both string and dictionary actions\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', 'unknown')\n",
        "                    action_types.append(action_type)\n",
        "\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        financial_actions.append(i)\n",
        "                elif isinstance(action, str):\n",
        "                    action_types.append(action)\n",
        "                    if action in ['transfer', 'payment']:\n",
        "                        financial_actions.append(i)\n",
        "\n",
        "            # Calculate financial action velocity\n",
        "            if len(financial_actions) > 1:\n",
        "                stats['financial_action_velocity'] = len(financial_actions) / (financial_actions[-1] - financial_actions[0] + 1)\n",
        "\n",
        "            # Action sequence complexity (entropy)\n",
        "            if action_types:\n",
        "                unique, counts = np.unique(action_types, return_counts=True)\n",
        "                probs = counts / len(action_types)\n",
        "                stats['action_sequence_complexity'] = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "            return stats\n",
        "        return {'action_sequence_complexity': 0, 'financial_action_velocity': 0, 'sensitive_action_clustering': 0}\n",
        "\n",
        "    action_sequence_analysis = sessions['Actions'].apply(analyze_action_sequences)\n",
        "    sequence_df = pd.DataFrame(action_sequence_analysis.tolist())\n",
        "    sequence_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    return sequence_df.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "def enhance_credit_behavior_features(panel_df, accounts):\n",
        "    \"\"\"Leverage your successful credit risk features\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Credit utilization patterns\n",
        "    credit_accounts = accounts[accounts['Type'] == 'credit_card']\n",
        "    if not credit_accounts.empty:\n",
        "        credit_behavior = credit_accounts.groupby('CustomerID').agg({\n",
        "            'Balance': ['mean', 'std', 'skew'],\n",
        "            'Limit': ['mean', 'min', 'max']\n",
        "        }).reset_index()\n",
        "        credit_behavior.columns = ['CustomerID', 'credit_balance_mean', 'credit_balance_std', 'credit_balance_skew',\n",
        "                                 'credit_limit_mean', 'credit_limit_min', 'credit_limit_max']\n",
        "\n",
        "        # Credit limit utilization patterns\n",
        "        credit_behavior['limit_utilization_variance'] = credit_behavior['credit_balance_std'] / (credit_behavior['credit_limit_mean'] + 1)\n",
        "        credit_behavior['limit_adequacy'] = credit_behavior['credit_limit_mean'] / (credit_behavior['credit_limit_max'] + 1)\n",
        "\n",
        "        features = features.merge(credit_behavior, on='CustomerID', how='left')\n",
        "\n",
        "    # Payment behavior sophistication\n",
        "    features['payment_consistency'] = 1 - features.groupby('CustomerID')['PaymentRatio'].transform('std').fillna(0)\n",
        "    features['utilization_stability'] = 1 - features.groupby('CustomerID')['Utilisation'].transform('std').fillna(0)\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def detect_behavioral_regimes(panel_df):\n",
        "    \"\"\"Detect different behavioral regimes in customer patterns\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Regime detection based on utilization patterns\n",
        "    features['high_utilization_regime'] = (\n",
        "        features['Utilisation'].rolling(3, min_periods=1).mean() > 0.7\n",
        "    ).astype(int)\n",
        "\n",
        "    features['deteriorating_regime'] = (\n",
        "        (features['Utilisation'].diff(2) > 0.1) &\n",
        "        (features['PaymentRatio'].diff(2) < -0.1)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Regime persistence\n",
        "    for customer_id in features['CustomerID'].unique():\n",
        "        cust_mask = features['CustomerID'] == customer_id\n",
        "        features.loc[cust_mask, 'regime_persistence'] = (\n",
        "            features.loc[cust_mask, 'high_utilization_regime'].rolling(4, min_periods=1).mean()\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "def enhance_temporal_features(panel_df):\n",
        "    \"\"\"Add more sophisticated time-series features\"\"\"\n",
        "    features = panel_df.copy()\n",
        "\n",
        "    # Seasonal decomposition\n",
        "    features['utilization_seasonal'] = features.groupby('CustomerID')['Utilisation'].transform(\n",
        "        lambda x: x - x.rolling(4, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "    # Change point detection\n",
        "    features['utilization_change_point'] = (\n",
        "        features['Utilisation'].rolling(3).std() > features['Utilisation'].rolling(6).std() * 1.5\n",
        "    ).astype(int)\n",
        "\n",
        "    # Momentum indicators\n",
        "    features['utilization_momentum'] = features['Utilisation'] - features['Utilisation'].shift(2)\n",
        "    features['payment_momentum'] = features['PaymentRatio'] - features['PaymentRatio'].shift(2)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_temporal_dynamics_features(panel_df):\n",
        "    \"\"\"Create sophisticated time-series features with trend analysis\"\"\"\n",
        "    temporal_features = []\n",
        "\n",
        "    for customer_id in panel_df['CustomerID'].unique():\n",
        "        cust_data = panel_df[panel_df['CustomerID'] == customer_id].sort_values('Week')\n",
        "\n",
        "        # Rolling statistics with multiple windows\n",
        "        for window in [2, 3, 4]:\n",
        "            cust_data[f'utilization_ma_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'payment_ratio_ma_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).mean()\n",
        "            cust_data[f'utilization_std_{window}'] = cust_data['Utilisation'].rolling(window, min_periods=1).std()\n",
        "            cust_data[f'payment_ratio_std_{window}'] = cust_data['PaymentRatio'].rolling(window, min_periods=1).std()\n",
        "\n",
        "        # Trend analysis\n",
        "        cust_data['utilization_trend_3'] = cust_data['Utilisation'].diff(periods=2).fillna(0)\n",
        "        cust_data['payment_trend_3'] = cust_data['PaymentRatio'].diff(periods=2).fillna(0)\n",
        "\n",
        "        # Acceleration (second derivative)\n",
        "        cust_data['utilization_acceleration'] = cust_data['utilization_trend_3'].diff().fillna(0)\n",
        "        cust_data['payment_acceleration'] = cust_data['payment_trend_3'].diff().fillna(0)\n",
        "\n",
        "        # Volatility measures\n",
        "        cust_data['utilization_volatility'] = cust_data['Utilisation'].rolling(4, min_periods=1).std()\n",
        "        cust_data['payment_volatility'] = cust_data['PaymentRatio'].rolling(4, min_periods=1).std()\n",
        "\n",
        "        # Behavioral patterns\n",
        "        cust_data['high_utilization_streak'] = (cust_data['Utilisation'] > 0.7).astype(int)\n",
        "        cust_data['low_payment_streak'] = (cust_data['PaymentRatio'] < 0.2).astype(int)\n",
        "\n",
        "        # Calculate streaks\n",
        "        for i in range(1, len(cust_data)):\n",
        "            if cust_data.iloc[i]['high_utilization_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('high_utilization_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['high_utilization_streak'] + 1\n",
        "            if cust_data.iloc[i]['low_payment_streak'] == 1:\n",
        "                cust_data.iloc[i, cust_data.columns.get_loc('low_payment_streak')] = \\\n",
        "                    cust_data.iloc[i-1]['low_payment_streak'] + 1\n",
        "\n",
        "        # Deterioration indicators\n",
        "        cust_data['financial_deterioration'] = (\n",
        "            (cust_data['utilization_trend_3'] > 0).astype(int) * 0.5 +\n",
        "            (cust_data['payment_trend_3'] < 0).astype(int) * 0.5\n",
        "        )\n",
        "\n",
        "        temporal_features.append(cust_data)\n",
        "\n",
        "    result = pd.concat(temporal_features, ignore_index=True).fillna(0)\n",
        "\n",
        "    # Apply enhanced temporal features\n",
        "    result = enhance_temporal_features(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_advanced_transaction_features(transactions_df):\n",
        "    \"\"\"Create comprehensive transaction behavior profiling\"\"\"\n",
        "    transactions = transactions_df.copy()\n",
        "    transactions['Timestamp'] = pd.to_datetime(transactions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Basic transaction metrics\n",
        "    txn_metrics = transactions.groupby('CustomerID').agg({\n",
        "        'TxnID': 'count',\n",
        "        'Amount': ['sum', 'mean', 'std', 'max', 'min', 'median', 'skew'],\n",
        "        'Timestamp': ['min', 'max', 'nunique']\n",
        "    }).reset_index()\n",
        "    txn_metrics.columns = ['CustomerID', 'txn_count', 'amount_sum', 'amount_mean', 'amount_std',\n",
        "                          'amount_max', 'amount_min', 'amount_median', 'amount_skew',\n",
        "                          'first_txn', 'last_txn', 'active_days']\n",
        "\n",
        "    # Transaction period and velocity\n",
        "    txn_metrics['txn_period_days'] = (txn_metrics['last_txn'] - txn_metrics['first_txn']).dt.days + 1\n",
        "    txn_metrics['daily_txn_frequency'] = txn_metrics['txn_count'] / txn_metrics['txn_period_days']\n",
        "    txn_metrics['daily_spending'] = txn_metrics['amount_sum'] / txn_metrics['txn_period_days']\n",
        "\n",
        "    # Large transaction analysis\n",
        "    large_txn_threshold = transactions['Amount'].quantile(0.8)\n",
        "    large_txns = transactions[transactions['Amount'] > large_txn_threshold]\n",
        "\n",
        "    if not large_txns.empty:\n",
        "        large_txn_stats = large_txns.groupby('CustomerID').agg({\n",
        "            'TxnID': 'count',\n",
        "            'Amount': ['mean', 'sum', 'max']\n",
        "        }).reset_index()\n",
        "        large_txn_stats.columns = ['CustomerID', 'large_txn_count', 'large_txn_avg',\n",
        "                                  'large_txn_sum', 'large_txn_max']\n",
        "\n",
        "        large_txn_stats['large_txn_ratio'] = large_txn_stats['large_txn_count'] / txn_metrics['txn_count']\n",
        "        large_txn_stats['large_amount_ratio'] = large_txn_stats['large_txn_sum'] / txn_metrics['amount_sum']\n",
        "    else:\n",
        "        large_txn_stats = pd.DataFrame(columns=['CustomerID', 'large_txn_ratio', 'large_amount_ratio'])\n",
        "\n",
        "    # Channel behavior\n",
        "    channel_behavior = pd.get_dummies(transactions[['CustomerID', 'Channel']],\n",
        "                                    columns=['Channel'], prefix='channel')\n",
        "    channel_behavior = channel_behavior.groupby('CustomerID').mean().reset_index()\n",
        "\n",
        "    # MCC spending patterns\n",
        "    if 'MCC_Group' in transactions.columns:\n",
        "        mcc_behavior = pd.get_dummies(transactions[['CustomerID', 'MCC_Group']],\n",
        "                                    columns=['MCC_Group'], prefix='mcc')\n",
        "        mcc_behavior = mcc_behavior.groupby('CustomerID').mean().reset_index()\n",
        "    else:\n",
        "        mcc_behavior = pd.DataFrame(columns=['CustomerID'])\n",
        "\n",
        "    # Temporal patterns\n",
        "    transactions['hour'] = transactions['Timestamp'].dt.hour\n",
        "    transactions['day_of_week'] = transactions['Timestamp'].dt.dayofweek\n",
        "    transactions['is_weekend'] = (transactions['day_of_week'] >= 5).astype(int)\n",
        "    transactions['is_night'] = ((transactions['hour'] >= 22) | (transactions['hour'] <= 6)).astype(int)\n",
        "\n",
        "    temporal_patterns = transactions.groupby('CustomerID').agg({\n",
        "        'is_weekend': 'mean',\n",
        "        'is_night': 'mean',\n",
        "        'hour': ['mean', 'std', lambda x: x.mode()[0] if len(x.mode()) > 0 else 12]\n",
        "    }).reset_index()\n",
        "    temporal_patterns.columns = ['CustomerID', 'weekend_ratio', 'night_ratio',\n",
        "                                'avg_txn_hour', 'std_txn_hour', 'mode_txn_hour']\n",
        "\n",
        "    # Spending consistency\n",
        "    daily_spending = transactions.groupby([transactions['Timestamp'].dt.date, 'CustomerID'])['Amount'].sum().reset_index()\n",
        "    spending_consistency = daily_spending.groupby('CustomerID')['Amount'].agg(['mean', 'std']).reset_index()\n",
        "    spending_consistency.columns = ['CustomerID', 'daily_spending_mean', 'daily_spending_std']\n",
        "    spending_consistency['spending_volatility'] = spending_consistency['daily_spending_std'] / (spending_consistency['daily_spending_mean'] + 1)\n",
        "\n",
        "    # Merge all features\n",
        "    features = txn_metrics.merge(channel_behavior, on='CustomerID', how='left')\n",
        "    if 'CustomerID' in mcc_behavior.columns:\n",
        "        features = features.merge(mcc_behavior, on='CustomerID', how='left')\n",
        "    features = features.merge(temporal_patterns, on='CustomerID', how='left')\n",
        "    features = features.merge(spending_consistency, on='CustomerID', how='left')\n",
        "\n",
        "    if 'CustomerID' in large_txn_stats.columns:\n",
        "        features = features.merge(large_txn_stats, on='CustomerID', how='left')\n",
        "\n",
        "    # Risk scores\n",
        "    features['transaction_risk_score'] = (\n",
        "        features.get('large_txn_ratio', 0) * 0.3 +\n",
        "        features['spending_volatility'] * 0.3 +\n",
        "        features['night_ratio'] * 0.2 +\n",
        "        (features['amount_skew'].abs() * 0.2)\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_session_intelligence_features(device_sessions):\n",
        "    \"\"\"Create advanced session behavior intelligence\"\"\"\n",
        "    sessions = device_sessions.copy()\n",
        "    sessions['Timestamp'] = pd.to_datetime(sessions['Timestamp'], format='ISO8601')\n",
        "\n",
        "    # Session frequency and patterns\n",
        "    session_freq = sessions.groupby('CustomerID').agg({\n",
        "        'SessionID': 'count',\n",
        "        'City': 'nunique',\n",
        "        'DeviceID': 'nunique',\n",
        "        'IP': 'nunique',\n",
        "        'Timestamp': ['min', 'max']\n",
        "    }).reset_index()\n",
        "    session_freq.columns = ['CustomerID', 'session_count', 'cities_visited', 'devices_used',\n",
        "                           'ips_used', 'first_session', 'last_session']\n",
        "\n",
        "    session_freq['session_period_days'] = (session_freq['last_session'] - session_freq['first_session']).dt.days + 1\n",
        "    session_freq['daily_sessions'] = session_freq['session_count'] / session_freq['session_period_days']\n",
        "\n",
        "    # Advanced action analysis - FIXED VERSION\n",
        "    def analyze_advanced_actions(actions_list):\n",
        "        if isinstance(actions_list, list):\n",
        "            stats = {\n",
        "                'total_actions': len(actions_list),\n",
        "                'financial_actions': 0,\n",
        "                'sensitive_actions': 0,\n",
        "                'login_count': 0,\n",
        "                'transfer_amount': 0,\n",
        "                'payment_amount': 0,\n",
        "                'unique_action_types': set()\n",
        "            }\n",
        "\n",
        "            for action in actions_list:\n",
        "                # Handle both string and dictionary actions\n",
        "                if isinstance(action, dict):\n",
        "                    action_type = action.get('type', '')\n",
        "                    stats['unique_action_types'].add(action_type)\n",
        "\n",
        "                    if action_type in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                        amount = action.get('amount', 0)\n",
        "                        if action_type == 'transfer':\n",
        "                            stats['transfer_amount'] += amount\n",
        "                        else:\n",
        "                            stats['payment_amount'] += amount\n",
        "                    elif action_type == 'account_view':\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                elif isinstance(action, str):\n",
        "                    stats['unique_action_types'].add(action)\n",
        "                    if action == 'login':\n",
        "                        stats['login_count'] += 1\n",
        "                    elif action in ['transfer', 'payment']:\n",
        "                        stats['financial_actions'] += 1\n",
        "                        stats['sensitive_actions'] += 1\n",
        "                    elif action == 'account_view':\n",
        "                        stats['sensitive_actions'] += 1\n",
        "\n",
        "            stats['unique_action_count'] = len(stats['unique_action_types'])\n",
        "            return stats\n",
        "        return {'total_actions': 0, 'financial_actions': 0, 'sensitive_actions': 0,\n",
        "                'login_count': 0, 'transfer_amount': 0, 'payment_amount': 0, 'unique_action_count': 0}\n",
        "\n",
        "    action_analysis = sessions['Actions'].apply(analyze_advanced_actions)\n",
        "    action_df = pd.DataFrame(action_analysis.tolist())\n",
        "    action_df['CustomerID'] = sessions['CustomerID'].values\n",
        "\n",
        "    # Aggregate action intelligence\n",
        "    action_intel = action_df.groupby('CustomerID').agg({\n",
        "        'total_actions': 'sum',\n",
        "        'financial_actions': 'sum',\n",
        "        'sensitive_actions': 'sum',\n",
        "        'login_count': 'sum',\n",
        "        'transfer_amount': 'sum',\n",
        "        'payment_amount': 'sum',\n",
        "        'unique_action_count': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate behavioral ratios\n",
        "    action_intel['financial_action_ratio'] = action_intel['financial_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['sensitive_action_ratio'] = action_intel['sensitive_actions'] / (action_intel['total_actions'] + 1)\n",
        "    action_intel['login_frequency'] = action_intel['login_count'] / session_freq['session_count']\n",
        "    action_intel['avg_financial_amount'] = (action_intel['transfer_amount'] + action_intel['payment_amount']) / (action_intel['financial_actions'] + 1)\n",
        "\n",
        "    # Merge features\n",
        "    features = session_freq.merge(action_intel, on='CustomerID', how='left')\n",
        "\n",
        "    # Security and risk indicators\n",
        "    features['geographic_dispersion'] = features['cities_visited'] / (features['session_count'] + 1)\n",
        "    features['device_diversity'] = features['devices_used'] / (features['session_count'] + 1)\n",
        "    features['ip_diversity'] = features['ips_used'] / (features['session_count'] + 1)\n",
        "\n",
        "    features['session_risk_score'] = (\n",
        "        features['geographic_dispersion'] * 0.3 +\n",
        "        features['device_diversity'] * 0.3 +\n",
        "        features['sensitive_action_ratio'] * 0.4\n",
        "    )\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "def create_interaction_features(features):\n",
        "    \"\"\"Create powerful interaction terms\"\"\"\n",
        "    # Financial stress interactions\n",
        "    if all(col in features.columns for col in ['utilization_ma_3', 'payment_ratio_ma_3']):\n",
        "        features['utilization_payment_interaction'] = (\n",
        "            features['utilization_ma_3'] * (1 - features['payment_ratio_ma_3'])\n",
        "        )\n",
        "\n",
        "    # Credit-behavior interactions\n",
        "    if all(col in features.columns for col in ['CreditScore', 'transaction_risk_score']):\n",
        "        features['credit_behavior_risk'] = (\n",
        "            (1 - features['CreditScore'] / 850) * features['transaction_risk_score']\n",
        "        )\n",
        "\n",
        "    # Multi-dimensional risk scoring\n",
        "    risk_components = []\n",
        "    risk_cols = ['high_risk_profile', 'transaction_risk_score', 'session_risk_score', 'financial_deterioration']\n",
        "\n",
        "    for col in risk_cols:\n",
        "        if col in features.columns:\n",
        "            risk_components.append(features[col])\n",
        "\n",
        "    if risk_components:\n",
        "        features['comprehensive_risk_index'] = sum(risk_components) / len(risk_components)\n",
        "\n",
        "    # Session-credit interactions\n",
        "    if all(col in features.columns for col in ['unique_action_count', 'CreditScore']):\n",
        "        features['session_credit_interaction'] = (\n",
        "            features['unique_action_count'] * (1 - features['CreditScore'] / 850)\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_elite_feature_matrix(panel_df, customer_df, temporal_df, transaction_df, session_df, enhanced_session_df):\n",
        "    \"\"\"Combine all elite features with intelligent interactions\"\"\"\n",
        "    features = temporal_df.copy()\n",
        "\n",
        "    # Merge all feature sources\n",
        "    features = features.merge(customer_df, on='CustomerID', how='left')\n",
        "    features = features.merge(transaction_df, on='CustomerID', how='left')\n",
        "    features = features.merge(session_df, on='CustomerID', how='left')\n",
        "    features = features.merge(enhanced_session_df, on='CustomerID', how='left')\n",
        "\n",
        "    # Enhanced credit behavior features\n",
        "    features = enhance_credit_behavior_features(features, accounts)\n",
        "\n",
        "    # Behavioral regime detection\n",
        "    features = detect_behavioral_regimes(features)\n",
        "\n",
        "    # Create powerful interaction features\n",
        "    # Financial stress indicators\n",
        "    stress_components = []\n",
        "    if 'utilization_ma_3' in features.columns:\n",
        "        stress_components.append(features['utilization_ma_3'] * 0.25)\n",
        "    if 'payment_ratio_ma_3' in features.columns:\n",
        "        stress_components.append((1 - features['payment_ratio_ma_3']) * 0.25)\n",
        "    if 'high_risk_profile' in features.columns:\n",
        "        stress_components.append(features['high_risk_profile'] * 0.25)\n",
        "    if 'transaction_risk_score' in features.columns:\n",
        "        stress_components.append(features['transaction_risk_score'] * 0.25)\n",
        "\n",
        "    if stress_components:\n",
        "        features['comprehensive_stress_score'] = sum(stress_components)\n",
        "\n",
        "    # Behavioral risk indicators\n",
        "    behavior_components = []\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        behavior_components.append(features['session_risk_score'] * 0.4)\n",
        "    if 'financial_deterioration' in features.columns:\n",
        "        behavior_components.append(features['financial_deterioration'] * 0.3)\n",
        "    if 'spending_volatility' in features.columns:\n",
        "        behavior_components.append(features['spending_volatility'] * 0.3)\n",
        "\n",
        "    if behavior_components:\n",
        "        features['behavioral_risk_score'] = sum(behavior_components)\n",
        "\n",
        "    # Credit capacity indicators\n",
        "    if 'CreditScore' in features.columns and 'credit_utilization' in features.columns:\n",
        "        features['credit_health_index'] = (\n",
        "            (features['CreditScore'] / 850) * 0.6 +\n",
        "            (1 - features['credit_utilization'].clip(0, 1)) * 0.4\n",
        "        )\n",
        "\n",
        "    # Payment behavior indicators\n",
        "    if 'payment_trend_3' in features.columns and 'payment_volatility' in features.columns:\n",
        "        features['payment_behavior_score'] = (\n",
        "            (1 - features['payment_trend_3'].clip(-1, 0).abs()) * 0.5 +\n",
        "            (1 - features['payment_volatility'].clip(0, 1)) * 0.5\n",
        "        )\n",
        "\n",
        "    # Apply interaction features\n",
        "    features = create_interaction_features(features)\n",
        "\n",
        "    # Final composite risk score\n",
        "    risk_components_final = []\n",
        "    if 'comprehensive_stress_score' in features.columns:\n",
        "        risk_components_final.append(features['comprehensive_stress_score'] * 0.3)\n",
        "    if 'behavioral_risk_score' in features.columns:\n",
        "        risk_components_final.append(features['behavioral_risk_score'] * 0.3)\n",
        "    if 'comprehensive_risk_index' in features.columns:\n",
        "        risk_components_final.append(features['comprehensive_risk_index'] * 0.2)\n",
        "    if 'session_risk_score' in features.columns:\n",
        "        risk_components_final.append(features['session_risk_score'] * 0.2)\n",
        "\n",
        "    if risk_components_final:\n",
        "        features['final_risk_score'] = sum(risk_components_final)\n",
        "\n",
        "    return features.fillna(0)\n",
        "\n",
        "print(\"ðŸ”„ Building stable feature sets...\")\n",
        "\n",
        "# Create all feature sets (same as before)\n",
        "customer_features = create_elite_customer_features(customers, accounts)\n",
        "temporal_train = create_temporal_dynamics_features(train_panel)\n",
        "temporal_test = create_temporal_dynamics_features(test_panel)\n",
        "\n",
        "# Combine train and test transactions for consistent feature engineering\n",
        "all_transactions = pd.concat([transactions_train, transactions_test])\n",
        "transaction_features = create_advanced_transaction_features(all_transactions)\n",
        "session_features = create_session_intelligence_features(device_sessions)\n",
        "enhanced_session_features = enhance_session_intelligence(device_sessions)\n",
        "\n",
        "print(\"ðŸŽ¯ Creating final feature matrix...\")\n",
        "\n",
        "# Create final datasets (same as before)\n",
        "X_train_elite = create_elite_feature_matrix(train_panel, customer_features, temporal_train, transaction_features, session_features, enhanced_session_features)\n",
        "X_test_elite = create_elite_feature_matrix(test_panel, customer_features, temporal_test, transaction_features, session_features, enhanced_session_features)\n",
        "\n",
        "# Prepare for modeling\n",
        "y_train = X_train_elite['DefaultLabel'].astype(int)\n",
        "non_feature_cols = ['CustomerID', 'Week', 'DefaultLabel', 'first_txn', 'last_txn', 'first_session', 'last_session']\n",
        "feature_cols = [col for col in X_train_elite.columns if col not in non_feature_cols]\n",
        "\n",
        "X_train = X_train_elite[feature_cols]\n",
        "X_test = X_test_elite[feature_cols]\n",
        "\n",
        "print(f\"âœ… Elite feature matrix: {X_train.shape[1]} features, {X_train.shape[0]} samples\")\n",
        "\n",
        "# Ensure numeric types and handle infinite values\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(\"ðŸ” Performing STABLE feature selection...\")\n",
        "\n",
        "# STABLE FEATURE SELECTION\n",
        "stable_selector = SelectFromModel(\n",
        "    RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "    threshold='median',  # More stable than top-k\n",
        "    max_features=60\n",
        ")\n",
        "stable_selector.fit(X_train, y_train)\n",
        "\n",
        "selected_mask = stable_selector.get_support()\n",
        "selected_features = X_train.columns[selected_mask]\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "print(f\"ðŸŽ¯ Selected {len(selected_features)} stable features\")\n",
        "\n",
        "# STABLE THRESHOLD OPTIMIZATION\n",
        "def stable_threshold_optimization(y_true, y_proba, n_bootstrap=50):\n",
        "    \"\"\"More stable threshold optimization with bootstrapping\"\"\"\n",
        "    thresholds = np.linspace(0.2, 0.5, 80)  # Wider range, more points\n",
        "    threshold_scores = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        bootstrap_scores = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            # Bootstrap sampling with fixed seed for each iteration\n",
        "            indices = np.random.RandomState(42).choice(len(y_true), len(y_true), replace=True)\n",
        "            y_true_boot = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
        "            y_proba_boot = y_proba[indices]\n",
        "\n",
        "            y_pred_boot = (y_proba_boot > threshold).astype(int)\n",
        "            score = f1_score(y_true_boot, y_pred_boot, average='macro')\n",
        "            bootstrap_scores.append(score)\n",
        "\n",
        "        threshold_scores.append(np.mean(bootstrap_scores))\n",
        "\n",
        "    best_idx = np.argmax(threshold_scores)\n",
        "    return thresholds[best_idx], threshold_scores[best_idx]\n",
        "\n",
        "# STABLE PREPROCESSING PIPELINE\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "print(\"ðŸ¤– Training STABLE ensemble model...\")\n",
        "\n",
        "# Apply stable preprocessing\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train_selected)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test_selected)\n",
        "\n",
        "# STABLE ENSEMBLE MODEL\n",
        "stable_ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(\n",
        "            n_estimators=400,\n",
        "            max_depth=25,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt',\n",
        "            class_weight='balanced_subsample',\n",
        "            bootstrap=True,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )),\n",
        "        ('gb', GradientBoostingClassifier(\n",
        "            n_estimators=250,\n",
        "            max_depth=7,\n",
        "            learning_rate=0.08,\n",
        "            subsample=0.85,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[0.65, 0.35]  # Fixed weights for stability\n",
        ")\n",
        "\n",
        "# Train the stable ensemble\n",
        "stable_ensemble.fit(X_train_processed, y_train)\n",
        "\n",
        "# Get probabilities\n",
        "y_train_proba = stable_ensemble.predict_proba(X_train_processed)[:, 1]\n",
        "y_test_proba = stable_ensemble.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "print(\"ðŸ“Š Performing STABLE threshold optimization...\")\n",
        "\n",
        "# Find optimal threshold\n",
        "optimal_threshold, optimal_f1 = stable_threshold_optimization(y_train, y_train_proba)\n",
        "\n",
        "print(f\"âœ… Optimal threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"âœ… Train F1 score: {optimal_f1:.4f}\")\n",
        "\n",
        "# Final predictions\n",
        "final_predictions = (y_test_proba > optimal_threshold).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'CustomerID': test_panel['CustomerID'],\n",
        "    'Week': test_panel['Week'],\n",
        "    'DefaultLabel': final_predictions\n",
        "})\n",
        "\n",
        "submission_filename = \"retailbanking_challenge2_stable_predictions.csv\"\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"ðŸŽ‰ STABLE predictions saved: {len(final_predictions)} predictions\")\n",
        "print(f\"   Default rate: {final_predictions.mean():.3f} ({final_predictions.sum()} defaults)\")\n",
        "\n",
        "# Model interpretation\n",
        "def explain_predictions(model, feature_names, top_n=20):\n",
        "    \"\"\"Provide business-interpretable feature importance\"\"\"\n",
        "    # For VotingClassifier, use the first estimator (RF) for feature importance\n",
        "    if hasattr(model, 'estimators_'):\n",
        "        rf_model = model.estimators_[0]\n",
        "        importances = rf_model.feature_importances_\n",
        "    else:\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': importances\n",
        "    }).sort_values('importance', ascending=False).head(top_n)\n",
        "\n",
        "    # Categorize features\n",
        "    categories = {\n",
        "        'Credit Risk': ['CreditScore', 'credit_utilization', 'high_risk_profile', 'credit_health'],\n",
        "        'Payment Behavior': ['PaymentRatio', 'payment_trend', 'payment_volatility', 'payment_consistency'],\n",
        "        'Spending Patterns': ['utilization_ma', 'transaction_risk_score', 'spending_volatility'],\n",
        "        'Session Behavior': ['session_risk_score', 'financial_actions', 'sensitive_action_ratio', 'action_sequence'],\n",
        "        'Behavioral Regimes': ['regime', 'deteriorating', 'persistence'],\n",
        "        'Composite Scores': ['comprehensive_risk', 'final_risk', 'stress_score']\n",
        "    }\n",
        "\n",
        "    for feature in importance_df['feature']:\n",
        "        category_found = False\n",
        "        for category, keywords in categories.items():\n",
        "            if any(keyword in feature for keyword in keywords):\n",
        "                importance_df.loc[importance_df['feature'] == feature, 'category'] = category\n",
        "                category_found = True\n",
        "                break\n",
        "        if not category_found:\n",
        "            importance_df.loc[importance_df['feature'] == feature, 'category'] = 'Other'\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "print(\"ðŸ” Top predictive features:\")\n",
        "feature_explanation = explain_predictions(stable_ensemble, selected_features)\n",
        "print(feature_explanation[['feature', 'category', 'importance']].head(15))\n",
        "\n",
        "# Submit predictions\n",
        "try:\n",
        "    from agentds import BenchmarkClient\n",
        "    client = BenchmarkClient(api_key=\"adsb_E8N9aNAz2w1K7dNYT8BSMGsd_1760199769\", team_name=\"synergy-minds\")\n",
        "\n",
        "    result = client.submit_prediction(\"Retailbanking\", 2, submission_filename)\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"ðŸ† STABLE Submission successful!\")\n",
        "        print(f\"   ðŸ“Š Score: {result['score']:.4f}\")\n",
        "        current_score = result['score']\n",
        "\n",
        "        if current_score >= 0.9500:\n",
        "            print(\"   ðŸŽ‰ EXCELLENT! Consistent elite performance achieved!\")\n",
        "        elif current_score >= 0.9450:\n",
        "            print(\"   ðŸ’ª Very good! Close to elite tier!\")\n",
        "        else:\n",
        "            improvement_needed = 0.9500 - current_score\n",
        "            print(f\"   ðŸŽ¯ Need improvement: {improvement_needed:.4f} to reach elite tier\")\n",
        "\n",
        "        # Track improvements\n",
        "        baseline_score = 0.4800\n",
        "        improvement = current_score - baseline_score\n",
        "        print(f\"   ðŸ“ˆ Overall improvement from baseline: +{improvement:.4f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Submission failed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ’¥ Submission error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ STABLE Strategy Features:\")\n",
        "print(\"   â€¢ Comprehensive random seed control\")\n",
        "print(\"   â€¢ Stable feature selection with median threshold\")\n",
        "print(\"   â€¢ Bootstrapped threshold optimization\")\n",
        "print(\"   â€¢ Fixed ensemble weights (no dynamic variability)\")\n",
        "print(\"   â€¢ Reproducible preprocessing pipeline\")\n",
        "print(\"   â€¢ Consistent data handling\")\n",
        "print(\"   â€¢ Enhanced model stability parameters\")\n",
        "\n",
        "# VALIDATION: Run multiple times to check consistency\n",
        "print(\"\\nðŸ” Running stability validation (3 quick runs)...\")\n",
        "validation_scores = []\n",
        "\n",
        "for i in range(3):\n",
        "    set_all_seeds(42 + i)  # Different seeds for validation\n",
        "\n",
        "    # Quick retrain with different seed to test stability\n",
        "    val_ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42+i)),\n",
        "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42+i))\n",
        "        ],\n",
        "        voting='soft',\n",
        "        weights=[0.65, 0.35]\n",
        "    )\n",
        "\n",
        "    val_ensemble.fit(X_train_processed, y_train)\n",
        "    y_val_proba = val_ensemble.predict_proba(X_train_processed)[:, 1]\n",
        "    val_threshold, val_f1 = stable_threshold_optimization(y_train, y_val_proba, n_bootstrap=10)\n",
        "    validation_scores.append(val_f1)\n",
        "    print(f\"   Run {i+1}: F1 = {val_f1:.4f}, Threshold = {val_threshold:.3f}\")\n",
        "\n",
        "print(f\"âœ… Stability check - Score range: {max(validation_scores):.4f} - {min(validation_scores):.4f}\")\n",
        "print(f\"   Consistency: \\u00b1{np.std(validation_scores):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}